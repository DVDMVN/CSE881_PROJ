{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72a5c19f",
   "metadata": {},
   "source": [
    "# Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43294e44",
   "metadata": {},
   "source": [
    "Step 1 - Read dict_data_type.xlsx as tables, remove variables with IS_KEEP = FALSE, cleanup  \n",
    "Step 2 - Merge data with diabetes_medication output  \n",
    "Step 3 - Split data into train-test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9648c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Statistics\n",
    "from scipy.stats import ks_2samp, chi2_contingency\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4258b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read tables\n",
    "init_selection_tab = pd.read_excel('SHEETS/dict_data_type.xlsx', sheet_name='init_selection')\n",
    "init_selection_tab.to_csv('TABLES/init_selection_tab.csv', index=False)\n",
    "drop_vars = init_selection_tab.loc[init_selection_tab[\"IS_KEEP\"] == False, \"variable_name\"]\n",
    "cat_vars = init_selection_tab.loc[init_selection_tab[\"IS_CATEGORICAL\"] == True, \"variable_name\"]\n",
    "\n",
    "# primary df\n",
    "dtypes = pd.read_csv(\"INPUTS/CSV/df_03_renamed_dtypes.csv\", index_col=0).squeeze(\"columns\").to_dict()\n",
    "df = pd.read_csv(\"INPUTS/CSV/df_03_renamed.csv\", dtype=dtypes)\n",
    "df = df.drop(columns=drop_vars)\n",
    "\n",
    "# diabetes medication\n",
    "diamed_dtypes = pd.read_csv(\"INPUTS/CSV/DIAMED_dtypes.csv\", index_col=0).squeeze(\"columns\").to_dict()\n",
    "diamed = pd.read_csv(\"INPUTS/CSV/DIAMED.csv\", dtype=diamed_dtypes)\n",
    "\n",
    "# merge\n",
    "df = df.merge(diamed, on=\"SEQN\", how=\"left\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# set categorical variables\n",
    "for var in cat_vars:\n",
    "    if var in df.columns: df[var] = df[var].astype(\"category\")\n",
    "\n",
    "# read table to set 7,77,9,99 etc to NaN\n",
    "# invalid_map = pd.read_csv(\"./PROCESSED/DATA/invalid_map_questionnaire_2017_2020.csv\")\n",
    "invalid_map = pd.read_csv(\"./PROCESSED/DATA/invalid_map_questionnaire_2017_2020.csv\", dtype=str)\n",
    "invalid_dict = invalid_map.groupby(\"variable\")[\"invalid_code\"].apply(list).to_dict()\n",
    "\n",
    "# loop and replace invalid (don't know, refused, etc.) with NaN\n",
    "# for var, codes in invalid_dict.items():\n",
    "#     match = [c for c in df.columns if c.startswith(var)]\n",
    "#     if not match: continue\n",
    "#     col = match[0]\n",
    "#     df[col] = df[col].replace(codes, pd.NA)\n",
    "\n",
    "for var, codes in invalid_dict.items():\n",
    "    cols = [c for c in df.columns if c == var or c.startswith(var)]\n",
    "    if not cols: continue\n",
    "    # match \"7\"/\"77\"/\".\" and also 7/77 (numbers)\n",
    "    codes_mixed = set(codes) | {int(c) for c in codes if c.isdigit()}\n",
    "    df[cols] = df[cols].mask(df[cols].isin(codes_mixed), pd.NA)\n",
    "\n",
    "del dtypes, diamed_dtypes, init_selection_tab, drop_vars, cat_vars, diamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b8b5ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train-test\n",
    "train_pre_cleaned, test_pre_cleaned = train_test_split(df, test_size=0.2, random_state=42)\n",
    "X = df.drop(columns=['LBXGH'])\n",
    "y = df[['LBXGH']]\n",
    "X_train_pre_cleaned, X_test_pre_cleaned, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "pd.DataFrame(X_test_pre_cleaned).to_csv(\"RESULTS/X_test_pre_cleaned.csv\", index=False)\n",
    "\n",
    "del X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390b8c27",
   "metadata": {},
   "source": [
    "# Imputation\n",
    "Random Forest imputation using sklearn iterativeimputer. MissForest doesn't allow to re-use the model to impute the test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98ab1c8",
   "metadata": {},
   "source": [
    "step 1 - encode both train/test   \n",
    "step 2 - train iterativeimputer random forest on train set only  \n",
    "step 3 - use trained imputation model on both train and test  \n",
    "Step 4 - Perform distribution test, Kolgomorov-Smirnov for continuous, Chi-square for categorical  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2537e166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data is encoded to ordinal encoder since iterative imputer only accepts numerical values\n",
    "# Trees can handle ordinal encoded categorical variables without issue\n",
    "# Using one-hot encoding would increase the number of features too much\n",
    "# However, linear models would require one-hot encoding to avoid implying ordinality\n",
    "# And also for distribution tests later, we need to decode back to original categories\n",
    "\n",
    "# named cat_cols instead of cat_vars to avoid confusion\n",
    "cat_cols = X_train_pre_cleaned.select_dtypes(include=['category']).columns.tolist()\n",
    "ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "X_train_ordinal = X_train_pre_cleaned.copy()\n",
    "X_test_ordinal = X_test_pre_cleaned.copy()\n",
    "X_test_ordinal[cat_cols] = ordinal_encoder.fit_transform(X_test_ordinal[cat_cols])\n",
    "X_train_ordinal[cat_cols] = ordinal_encoder.transform(X_train_ordinal[cat_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b54816f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\victo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\impute\\_iterative.py:825: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# For a random forest imputer, we don't really need a huge number of trees\n",
    "# Imputation is about generating stable estimates, not prediction accuracy\n",
    "# So we can limit the number of trees to speed up computation\n",
    "# Usually n=10-50 is sufficient, but we can go a bit higher if it's unstable\n",
    "def random_forest_imputer(n_estimators=20, random_state=42):\n",
    "    iterative_imputer = IterativeImputer(\n",
    "        estimator=RandomForestRegressor(n_estimators=n_estimators, max_depth=10, n_jobs=-1, random_state=random_state),\n",
    "        max_iter=5,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    return iterative_imputer\n",
    "\n",
    "X_train = random_forest_imputer(n_estimators=20, random_state=42).fit_transform(X_train_ordinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c918ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2 = random_forest_imputer(n_estimators=20, random_state=0).fit_transform(X_train_ordinal)\n",
    "diff = np.abs(X_train - X_train2)\n",
    "print(f'Max difference between two imputations on train set with different random states: {diff.max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278be0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = random_forest_imputer(n_estimators=20, random_state=42).fit_transform(X_test_ordinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454eecd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform distribution test, Kolgomorov-Smirnov for continuous, Chi-square for categorical\n",
    "\n",
    "ks_results = []\n",
    "chi2_results = []\n",
    "\n",
    "for col in X_train.columns:\n",
    "    if col == \"LBXGH\":\n",
    "        continue\n",
    "\n",
    "    # if str(X_train[col].dtype) == \"category\":\n",
    "    #     # Chi-square test for categorical\n",
    "    #     contingency = pd.crosstab(X_train[col], X_test[col])\n",
    "\n",
    "    #     # skip if no valid data for chi-square\n",
    "    #     if contingency.size == 0 or contingency.shape[0] < 2 or contingency.shape[1] < 2: continue\n",
    "\n",
    "    #     chi2, p, dof, expected = chi2_contingency(contingency)\n",
    "    #     chi2_results.append({\"variable\": col, \"Chi2_stat\": chi2, \"p_value\": p})\n",
    "\n",
    "    if str(X_train[col].dtype) == \"category\":\n",
    "        train_counts = X_train[col].value_counts(dropna=False)\n",
    "        test_counts = X_test[col].value_counts(dropna=False)\n",
    "        # cats = sorted(set(train_counts.index) | set(test_counts.index))\n",
    "        cats = list(set(train_counts.index) | set(test_counts.index))\n",
    "        contingency = pd.DataFrame({\n",
    "            \"train\": train_counts.reindex(cats, fill_value=0),\n",
    "            \"test\": test_counts.reindex(cats, fill_value=0)\n",
    "        }).T\n",
    "\n",
    "    if contingency.shape[1] >= 2:  # need at least two categories\n",
    "        chi2, p, dof, expected = chi2_contingency(contingency)\n",
    "        chi2_results.append({\"variable\": col, \"Chi2_stat\": chi2, \"p_value\": p})\n",
    "\n",
    "\n",
    "    else:\n",
    "        # KS test for continuous\n",
    "        ks_stat, ks_p = ks_2samp(X_train[col].dropna(), X_test[col].dropna())\n",
    "        ks_results.append({\"variable\": col, \"KS_stat\": ks_stat, \"p_value\": ks_p})\n",
    "\n",
    "\n",
    "ks_results_df = pd.DataFrame(ks_results)\n",
    "chi2_results_df = pd.DataFrame(chi2_results)\n",
    "\n",
    "# KS summary\n",
    "total_ks = len(ks_results_df)\n",
    "n_sig_ks = (ks_results_df[\"p_value\"] < 0.05).sum()\n",
    "pct_sig_ks = n_sig_ks / total_ks * 100\n",
    "print(f\"KS test: {n_sig_ks} variables ({pct_sig_ks:.2f}%) have p < 0.05\")\n",
    "\n",
    "# Chi-square summary\n",
    "total_chi2 = len(chi2_results_df)\n",
    "n_sig_chi2 = (chi2_results_df[\"p_value\"] < 0.05).sum()\n",
    "pct_sig_chi2 = n_sig_chi2 / total_chi2 * 100\n",
    "print(f\"Chi-square test: {n_sig_chi2} variables ({pct_sig_chi2:.2f}%) have p < 0.05\")\n",
    "\n",
    "\n",
    "# Save results for audit purposes\n",
    "pd.DataFrame(ks_results).to_excel(\"RESULTS/KS_results.xlsx\", index=False)\n",
    "pd.DataFrame(chi2_results).to_excel(\"RESULTS/Chi2_results.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
