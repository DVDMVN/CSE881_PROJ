{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6cf06c8",
   "metadata": {},
   "source": [
    "# Load Packages and Merged Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "795bca0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "\n",
    "merged = pd.read_parquet(Path(\"PROCESSED/DATA/merged.parquet\")) # From previous step (B_MERGE_DATA)\n",
    "merged = merged.replace(['', ' ', '.'], pd.NA)  # Handle blanks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5c630f",
   "metadata": {},
   "source": [
    "### Step 1: Transforming Numerical Missing to True Missing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98883a8b",
   "metadata": {},
   "source": [
    "Getting all associated numerical codes associated with missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2232ea01",
   "metadata": {},
   "outputs": [],
   "source": [
    "INVALID_PATTERN = re.compile(\n",
    "    r\"(?:refused|don['']?t\\s*know|missing|blank but applicable|\"\n",
    "    r\"(?:can|could)\\s*not\\s*assess|unknown|not\\s*ascertained)\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "def get_invalid_codes(codebook):\n",
    "    result = {}\n",
    "    codebook_df = pd.read_csv(codebook)\n",
    "    if 'variable_name' not in codebook_df.columns:\n",
    "        return\n",
    "    for var_name in codebook_df['variable_name']:\n",
    "        table = codebook_df[codebook_df['variable_name'] == var_name]['table'].item()\n",
    "        if type(table) is float:\n",
    "            continue\n",
    "\n",
    "        df = pd.read_html(StringIO(table))[0]\n",
    "        \n",
    "\n",
    "        if \"Code or Value\" in df.columns and \"Value Description\" in df.columns:\n",
    "            # rows with invalid desc\n",
    "            # regex warning handled by using non-capturing groups above\n",
    "            invalid = df[\"Value Description\"].astype(str).str.contains(INVALID_PATTERN, na=False)\n",
    "            if invalid.any():\n",
    "                codes = df.loc[invalid, \"Code or Value\"].astype(str).str.strip().tolist()\n",
    "                result[var_name] = codes\n",
    "    return result\n",
    "\n",
    "# Populate \"invalid_map\" (variable name to invalid codes) for questionnaire_data\n",
    "questionnaire_codebooks_path = Path(\"RAW/CODEBOOKS/questionnaire_data\")\n",
    "codebook_files = list(questionnaire_codebooks_path.rglob(\"*.csv\"))\n",
    "\n",
    "invalid_map = {}\n",
    "for codebook_file in codebook_files:\n",
    "    invalid_codes = get_invalid_codes(codebook_file)\n",
    "    if invalid_codes:\n",
    "        invalid_map.update(invalid_codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a948ff5",
   "metadata": {},
   "source": [
    "Saving the mapper to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d28cefcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved invalid_map_questionnaire_2017_2020.csv to ./PROCESSED/DATA/invalid_map_questionnaire_2017_2020.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the invalid map to a CSV file\n",
    "df_invalid_map = pd.DataFrame([\n",
    "    {'variable': var, 'invalid_code': code}\n",
    "    for var, codes in invalid_map.items()\n",
    "    for code in codes\n",
    "])\n",
    "\n",
    "directory = \"./PROCESSED/DATA\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "save_path = directory + \"/invalid_map_questionnaire_2017_2020.csv\"\n",
    "df_invalid_map.to_csv(save_path, index = False)\n",
    "print(f\"Saved invalid_map_questionnaire_2017_2020.csv to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d5c797",
   "metadata": {},
   "source": [
    "Apply transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b014ce0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\victo\\AppData\\Local\\Temp\\ipykernel_44536\\1495827585.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[\"BP_sys_median\"] = merged[bp_sys].median(axis=1)\n",
      "C:\\Users\\victo\\AppData\\Local\\Temp\\ipykernel_44536\\1495827585.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[\"BP_dia_median\"] = merged[bp_dia].median(axis=1)\n",
      "C:\\Users\\victo\\AppData\\Local\\Temp\\ipykernel_44536\\1495827585.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[\"Pulse_median\"] = merged[bp_pulse].median(axis=1)\n"
     ]
    }
   ],
   "source": [
    "# read table to set 7,77,9,99 etc to NaN\n",
    "# invalid_map = pd.read_csv(\"./PROCESSED/DATA/invalid_map_questionnaire_2017_2020.csv\")\n",
    "invalid_map = pd.read_csv(\"./PROCESSED/DATA/invalid_map_questionnaire_2017_2020.csv\", dtype=str)\n",
    "invalid_dict = invalid_map.groupby(\"variable\")[\"invalid_code\"].apply(list).to_dict()\n",
    " \n",
    "# loop and replace invalid (don't know, refused, etc.) with NaN\n",
    "# for var, codes in invalid_dict.items():\n",
    "#     match = [c for c in df.columns if c.startswith(var)]\n",
    "#     if not match: continue\n",
    "#     col = match[0]\n",
    "#     df[col] = df[col].replace(codes, pd.NA)\n",
    " \n",
    "for var, codes in invalid_dict.items():\n",
    "    cols = [c for c in merged.columns if c == var or c.endswith(var)]\n",
    "    # print(cols)\n",
    "    if not cols: \n",
    "        continue\n",
    "    # match \"7\"/\"77\"/\".\" and also 7/77 (numbers)\n",
    "    codes_mixed = set(codes) | {int(c) for c in codes if c.isdigit()}\n",
    "    merged[cols] = merged[cols].mask(merged[cols].isin(codes_mixed), pd.NA)\n",
    "\n",
    "\n",
    "# create median blood pressure and pulse columns\n",
    "bp_sys = [\"P_BPXO__BPXOSY1\", \"P_BPXO__BPXOSY2\", \"P_BPXO__BPXOSY3\"]\n",
    "bp_dia = [\"P_BPXO__BPXODI1\", \"P_BPXO__BPXODI2\", \"P_BPXO__BPXODI3\"]\n",
    "bp_pulse = [\"P_BPXO__BPXOPLS1\", \"P_BPXO__BPXOPLS2\", \"P_BPXO__BPXOPLS3\"]\n",
    "\n",
    "merged[\"BP_sys_median\"] = merged[bp_sys].median(axis=1)\n",
    "merged[\"BP_dia_median\"] = merged[bp_dia].median(axis=1)\n",
    "merged[\"Pulse_median\"] = merged[bp_pulse].median(axis=1)\n",
    "\n",
    "merged = merged.drop(columns=bp_sys + bp_dia + bp_pulse, errors=\"ignore\")\n",
    "del bp_sys, bp_dia, bp_pulse\n",
    "\n",
    "\n",
    "# set DBD895 and DBD900 column value of 5555 to 22, simplification since not many are above 21\n",
    "merged[['P_DBQ__DBD895', 'P_DBQ__DBD900']] = merged[['P_DBQ__DBD895', 'P_DBQ__DBD900']].replace(5555, 22)\n",
    "\n",
    "# set DBD905 and DBD910 column value of 6666 to 91, simplification since not many are above 90\n",
    "merged[['P_DBQ__DBD905', 'P_DBQ__DBD910']] = merged[['P_DBQ__DBD905', 'P_DBQ__DBD910']].replace(6666, 91)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca4ffb2",
   "metadata": {},
   "source": [
    "### Step 2: Remove the columns with more than 30% missingness  \n",
    "\n",
    "This is a rudamentary drop. Our intention is to make a more granular pass later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "540a18bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 1209 columns with >30% missingness.\n",
      "Dropped 672 rows with missing target variable (LBXGH).\n",
      "Cleaning log saved to C:\\Users\\victo\\Documents\\CSE 881\\PROJECT\\CSE881_PROJ\\LOG\\log_cleaning.txt\n"
     ]
    }
   ],
   "source": [
    "# Step 2a - Remove columns with more than 30% missingness\n",
    "num_rows = merged.shape[0]\n",
    "threshold = 0.3 * num_rows\n",
    "\n",
    "cols_to_drop = merged.columns[merged.isnull().sum() > threshold]\n",
    "merged_dropped = merged.drop(columns=cols_to_drop)\n",
    "\n",
    "# Step 2b - Remove rows with no target variable\n",
    "target_col = 'LBXGH'\n",
    "before_rows = merged_dropped.shape[0]\n",
    "merged_dropped = merged_dropped.dropna(subset=[target_col])\n",
    "after_rows = merged_dropped.shape[0]\n",
    "rows_dropped = before_rows - after_rows\n",
    "\n",
    "# Logging the cleaning steps\n",
    "log_path = Path(\"LOG/log_cleaning.txt\")\n",
    "with open(log_path, \"w\") as log:\n",
    "    log.write(\"==== Data Cleaning Log ====\\n\\n\")\n",
    "\n",
    "    # Log columns dropped\n",
    "    log.write(f\"Columns dropped (>30% missingness): {len(cols_to_drop)}\\n\")\n",
    "    if len(cols_to_drop) > 0:\n",
    "        log.write(\"\\n\".join(cols_to_drop))\n",
    "        log.write(\"\\n\\n\")\n",
    "    else:\n",
    "        log.write(\"None\\n\\n\")\n",
    "\n",
    "    # Log rows dropped\n",
    "    log.write(f\"Rows dropped with missing target variable ({target_col}): {rows_dropped}\\n\")\n",
    "\n",
    "print(f\"Dropped {len(cols_to_drop)} columns with >30% missingness.\")\n",
    "print(f\"Dropped {rows_dropped} rows with missing target variable ({target_col}).\")\n",
    "print(f\"Cleaning log saved to {log_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e051d5",
   "metadata": {},
   "source": [
    "### Step 3: Selective Categorical Encoding and Dropping\n",
    "\n",
    "Manual screening was done for more granular dropping and encoding measures\n",
    "- `SHEETS/dict_data_type.xlsx`\n",
    "- `TABLES/init_selection_tab.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72a2e58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables before drop: 421\n",
      "Variables after drop: 258\n",
      "Total variables dropped: 163\n"
     ]
    }
   ],
   "source": [
    "init_selection_tab = pd.read_excel('SHEETS/dict_data_type.xlsx', sheet_name='init_selection')\n",
    "init_selection_tab.to_csv('TABLES/init_selection_tab.csv', index=False)\n",
    "drop_vars = init_selection_tab.loc[init_selection_tab[\"IS_KEEP\"] == False, \"variable_name\"]\n",
    "cat_vars = init_selection_tab.loc[init_selection_tab[\"IS_CATEGORICAL\"] == True, \"variable_name\"]\n",
    "\n",
    "drop_vars = drop_vars.str.split('_').str[0] # Extract variable label\n",
    "cat_vars = cat_vars.str.split('_').str[0] # Extract variable labelcat_vars\n",
    "merged_base_names = merged_dropped.columns.to_series().str.split('__').str[-1] # Extract variable label\n",
    "\n",
    "# Track variables before drop\n",
    "before_drop = merged_dropped.shape[1]\n",
    "print(f\"Variables before drop: {before_drop}\")\n",
    "\n",
    "# Perform special selected drop\n",
    "drop_vars_mask = merged_base_names.isin(drop_vars)\n",
    "merged_dropped = merged_dropped.drop(\n",
    "    columns = merged_dropped.columns[drop_vars_mask]\n",
    ")\n",
    "\n",
    "# Track variables after drop\n",
    "after_drop = merged_dropped.shape[1]\n",
    "print(f\"Variables after drop: {after_drop}\")\n",
    "print(f\"Total variables dropped: {before_drop - after_drop}\")\n",
    "\n",
    "# Perform special categorical encoding\n",
    "merged_base_names = merged_dropped.columns.to_series().str.split('__').str[-1]\n",
    "# merged_base_names = merged_base_names.reset_index(drop = True) # taken out, breaks its original index alignment with merged_dropped.columns\n",
    "# for var in cat_vars:\n",
    "    # if var in merged_base_names.values:\n",
    "    #     idx = merged_base_names.index[merged_base_names.values == var]\n",
    "    #     column_name = merged_dropped.columns[idx]\n",
    "    #     merged_dropped[column_name] = merged_dropped[column_name].astype(\"category\")\n",
    "\n",
    "# cast one column at a time to avoid the “mixed block” issue\n",
    "for col, base in zip(merged_dropped.columns, merged_base_names):\n",
    "    if base in set(cat_vars):\n",
    "        merged_dropped[col] = merged_dropped[col].astype('category')\n",
    "\n",
    "dtypes = merged_dropped.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18acda79",
   "metadata": {},
   "source": [
    "## Rename for Clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cd13ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved dictionary to: C:\\Users\\victo\\Documents\\CSE 881\\PROJECT\\CSE881_PROJ\\TABLES\\dictionary.csv\n"
     ]
    }
   ],
   "source": [
    "# Folder with all codebooks\n",
    "codebook_root = Path(\"RAW/CODEBOOKS\")\n",
    "\n",
    "# Helper to clean symbols\n",
    "def clean_text(s):\n",
    "    s = \"\" if pd.isna(s) else str(s)\n",
    "    s = re.sub(r\"[^A-Za-z0-9_]+\", \"_\", s)\n",
    "    return re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "\n",
    "# Collect all codebook files\n",
    "codebook_list = []\n",
    "for f in codebook_root.rglob(\"*_codebook.csv\"):\n",
    "    df = pd.read_csv(f)\n",
    "    df.columns = [c.lower().replace(\" \", \"_\") for c in df.columns]\n",
    "    df = df.rename(columns={\"variable_name\": \"var\", \"variable\": \"var\",\n",
    "                            \"sas_label\": \"label\", \"label\": \"label\"})\n",
    "    if \"var\" not in df or \"label\" not in df:\n",
    "        continue\n",
    "    data_file = Path(f).stem.replace(\"_codebook\", \"\")\n",
    "    df[\"variable_name\"] = df[\"var\"].apply(clean_text)\n",
    "    df[\"sas_label\"] = df[\"label\"].apply(clean_text)\n",
    "    df[\"variable_label\"] = df[\"variable_name\"] + \"_\" + df[\"sas_label\"]\n",
    "    df[\"data_file\"] = data_file\n",
    "    codebook_list.append(df[[\"data_file\", \"variable_name\", \"sas_label\", \"variable_label\"]])\n",
    "\n",
    "# Combine and save\n",
    "if codebook_list:\n",
    "    out = pd.concat(codebook_list, ignore_index=True)\n",
    "    output_path = Path(\"TABLES/dictionary.csv\")\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    out.to_csv(output_path, index=False)\n",
    "    print(f\"Saved dictionary to: {output_path.resolve()}\")\n",
    "else:\n",
    "    print(\"No valid codebooks found.\")\n",
    "\n",
    "dict_path = \"TABLES/dictionary.csv\"\n",
    "df_dict = pd.read_csv(dict_path)\n",
    "\n",
    "# Mapping: {old_name: new_name}\n",
    "# The old name is data_file + \"__\" + variable_name (same as in merged data)\n",
    "# df_dict[\"old_name\"] = df_dict[\"data_file\"] + \"__\" + df_dict[\"variable_name\"]\n",
    "# rename_map = dict(zip(df_dict[\"old_name\"], df_dict[\"variable_label\"]))\n",
    "\n",
    "# Modified dictionary mapping that preserves file prefix\n",
    "df_dict[\"old_name\"] = df_dict[\"data_file\"] + \"__\" + df_dict[\"variable_name\"]\n",
    "df_dict[\"new_name\"] = df_dict[\"data_file\"] + \"__\" + df_dict[\"variable_label\"]\n",
    "rename_map = dict(zip(df_dict[\"old_name\"], df_dict[\"new_name\"]))\n",
    "\n",
    "# Apply renaming\n",
    "merged_dropped = merged_dropped.rename(columns=rename_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5de9f15",
   "metadata": {},
   "source": [
    "### Save to .parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "907288a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved merged_and_dropped.parquet to ./PROCESSED/DATA/merged_and_dropped.parquet\n"
     ]
    }
   ],
   "source": [
    "directory = \"./PROCESSED/DATA\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "save_path = directory + \"/merged_and_dropped.parquet\"\n",
    "\n",
    "cat_cols = merged_dropped.select_dtypes(include=['category']).columns.tolist()\n",
    "with open(\"./PROCESSED/DATA/merged_and_dropped.cat_cols.json\", \"w\") as f:\n",
    "    json.dump(cat_cols, f)\n",
    "\n",
    "merged_dropped.to_parquet(save_path, index = False)\n",
    "merged_dropped.to_csv(directory + \"/merged_and_dropped.csv\", index = False)\n",
    "print(f\"Saved merged_and_dropped.parquet to {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
