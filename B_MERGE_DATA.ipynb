{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62adaa90",
   "metadata": {},
   "source": [
    "# Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7fe1753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054cb86c",
   "metadata": {},
   "source": [
    "# Perform Merge onto `P_GHB.xpt` (Target File)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a12d7d2",
   "metadata": {},
   "source": [
    "Merging all files together to base `P_GHB.xpt`:\n",
    "- P_GHB contains our target, so this will be the base of our merging operations\n",
    "- ⚠️ Skipping 9 specific files with duplicate `SEQN`:\n",
    "    - dietary_data: The dietary data large contains some diet information over the course of two tracked days, this information is not particularly insightful for our case, since diabetes is a longform developing disease.\n",
    "        - `P_DR1IFF.xpt`\n",
    "        - `P_DR2IFF.xpt`\n",
    "        - `P_DS1IDS.xpt`\n",
    "        - `P_DS2IDS.xpt`\n",
    "        - `P_DSQIDS.xpt`\n",
    "    - examination_data: These files deal with audio sensor data, which is not useful for our use case.\n",
    "        - `P_AUXAR.xpt`\n",
    "        - `P_AUXTYM.xpt`\n",
    "        - `P_AUXWBR.xpt`\n",
    "    - questionnaire_data: This data needs to undergo transformation before being usable. We first transform this dataset before merging.\n",
    "        - `P_RXQ_RX.xpt`\n",
    "\n",
    "- Skipping additional dietary data files:\n",
    "    - `P_DR1TOT.xpt`,\n",
    "    - `P_DR2TOT.xpt`,\n",
    "    - `P_DS1TOT.xpt`,\n",
    "    - `P_DS2TOT.xpt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e305eb27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: P_DEMO.xpt\n",
      "Loading: DSBI.xpt\n",
      "\tSkipping DSBI.xpt (manually excluded)\n",
      "Loading: DSII.xpt\n",
      "\tSkipping DSII.xpt (manually excluded)\n",
      "Loading: DSPI.xpt\n",
      "\tSkipping DSPI.xpt (manually excluded)\n",
      "Loading: P_DR1IFF.xpt\n",
      "\tSkipping P_DR1IFF.xpt (manually excluded)\n",
      "Loading: P_DR1TOT.xpt\n",
      "\tSkipping P_DR1TOT.xpt (manually excluded)\n",
      "Loading: P_DR2IFF.xpt\n",
      "\tSkipping P_DR2IFF.xpt (manually excluded)\n",
      "Loading: P_DR2TOT.xpt\n",
      "\tSkipping P_DR2TOT.xpt (manually excluded)\n",
      "Loading: P_DRXFCD.xpt\n",
      "\tSkipping P_DRXFCD.xpt (manually excluded)\n",
      "Loading: P_DS1IDS.xpt\n",
      "\tSkipping P_DS1IDS.xpt (manually excluded)\n",
      "Loading: P_DS1TOT.xpt\n",
      "\tSkipping P_DS1TOT.xpt (manually excluded)\n",
      "Loading: P_DS2IDS.xpt\n",
      "\tSkipping P_DS2IDS.xpt (manually excluded)\n",
      "Loading: P_DS2TOT.xpt\n",
      "\tSkipping P_DS2TOT.xpt (manually excluded)\n",
      "Loading: P_DSQIDS.xpt\n",
      "\tSkipping P_DSQIDS.xpt (manually excluded)\n",
      "Loading: P_DSQTOT.xpt\n",
      "\tSkipping P_DSQTOT.xpt (manually excluded)\n",
      "Loading: P_AUX.xpt\n",
      "Loading: P_AUXAR.xpt\n",
      "\tSkipping P_AUXAR.xpt (manually excluded)\n",
      "Loading: P_AUXTYM.xpt\n",
      "\tSkipping P_AUXTYM.xpt (manually excluded)\n",
      "Loading: P_AUXWBR.xpt\n",
      "\tSkipping P_AUXWBR.xpt (manually excluded)\n",
      "Loading: P_BMX.xpt\n",
      "Loading: P_BPXO.xpt\n",
      "Loading: P_DXXFEM.xpt\n",
      "Loading: P_DXXSPN.xpt\n",
      "Loading: P_LUX.xpt\n",
      "Loading: P_OHXDEN.xpt\n",
      "Loading: P_OHXREF.xpt\n",
      "Loading: P_ALB_CR.xpt\n",
      "Loading: P_BIOPRO.xpt\n",
      "Loading: P_CBC.xpt\n",
      "Loading: P_CMV.xpt\n",
      "Loading: P_COT.xpt\n",
      "Loading: P_CRCO.xpt\n",
      "Loading: P_ETHOX.xpt\n",
      "Loading: P_FASTQX.xpt\n",
      "Loading: P_FERTIN.xpt\n",
      "Loading: P_FETIB.xpt\n",
      "Loading: P_FOLATE.xpt\n",
      "Loading: P_FOLFMS.xpt\n",
      "Loading: P_FR.xpt\n",
      "Loading: P_GHB.xpt\n",
      "\tSkipping P_GHB.xpt (manually excluded)\n",
      "Loading: P_GLU.xpt\n",
      "Loading: P_HDL.xpt\n",
      "Loading: P_HEPA.xpt\n",
      "Loading: P_HEPBD.xpt\n",
      "Loading: P_HEPB_S.xpt\n",
      "Loading: P_HEPC.xpt\n",
      "Loading: P_HEPE.xpt\n",
      "Loading: P_HSCRP.xpt\n",
      "Loading: P_IHGEM.xpt\n",
      "Loading: P_INS.xpt\n",
      "Loading: P_OPD.xpt\n",
      "Loading: P_PBCD.xpt\n",
      "Loading: P_PERNT.xpt\n",
      "Loading: P_PFAS.xpt\n",
      "Loading: P_SSAGP.xpt\n",
      "Loading: P_SSFR.xpt\n",
      "Loading: P_TCHOL.xpt\n",
      "Loading: P_TFR.xpt\n",
      "Loading: P_TRIGLY.xpt\n",
      "Loading: P_TST.xpt\n",
      "Loading: P_UAS.xpt\n",
      "Loading: P_UCFLOW.xpt\n",
      "Loading: P_UCM.xpt\n",
      "Loading: P_UCPREG.xpt\n",
      "Loading: P_UHG.xpt\n",
      "Loading: P_UIO.xpt\n",
      "Loading: P_UM.xpt\n",
      "Loading: P_UNI.xpt\n",
      "Loading: P_UTAS.xpt\n",
      "Loading: P_UVOC.xpt\n",
      "Loading: P_UVOC2.xpt\n",
      "Loading: P_VOCWB.xpt\n",
      "Loading: P_ACQ.xpt\n",
      "Loading: P_ALQ.xpt\n",
      "Loading: P_AUQ.xpt\n",
      "Loading: P_BPQ.xpt\n",
      "Loading: P_CBQPFA.xpt\n",
      "Loading: P_CBQPFC.xpt\n",
      "Loading: P_CDQ.xpt\n",
      "Loading: P_DBQ.xpt\n",
      "Loading: P_DEQ.xpt\n",
      "Loading: P_DIQ.xpt\n",
      "Loading: P_DPQ.xpt\n",
      "Loading: P_ECQ.xpt\n",
      "Loading: P_FSQ.xpt\n",
      "Loading: P_HEQ.xpt\n",
      "Loading: P_HIQ.xpt\n",
      "Loading: P_HSQ.xpt\n",
      "Loading: P_HUQ.xpt\n",
      "Loading: P_IMQ.xpt\n",
      "Loading: P_INQ.xpt\n",
      "Loading: P_KIQ_U.xpt\n",
      "Loading: P_MCQ.xpt\n",
      "Loading: P_OCQ.xpt\n",
      "Loading: P_OHQ.xpt\n",
      "Loading: P_OSQ.xpt\n",
      "Loading: P_PAQ.xpt\n",
      "Loading: P_PAQY.xpt\n",
      "Loading: P_PUQMEC.xpt\n",
      "Loading: P_RHQ.xpt\n",
      "Loading: P_RXQASA.xpt\n",
      "Loading: P_RXQ_RX.xpt\n",
      "\tSkipping P_RXQ_RX.xpt (manually excluded)\n",
      "Loading: P_SLQ.xpt\n",
      "Loading: P_SMQ.xpt\n",
      "Loading: P_SMQFAM.xpt\n",
      "Loading: P_SMQRTU.xpt\n",
      "Loading: P_SMQSHS.xpt\n",
      "Loading: P_VTQ.xpt\n",
      "Loading: P_WHQ.xpt\n",
      "Loading: P_WHQMEC.xpt\n",
      "Loading: RXQ_DRUG.xpt\n",
      "\tSkipping RXQ_DRUG.xpt, missing SEQN\n",
      "\n",
      "Merging complete. Duplicate column log saved to C:\\Users\\victo\\Documents\\CSE 881\\PROJECT\\CSE881_PROJ\\LOG\\log_merge.txt\n"
     ]
    }
   ],
   "source": [
    "root = Path(\"RAW/DATA\")\n",
    "xpt_files = list(root.rglob(\"*.xpt\"))\n",
    "\n",
    "P_GHB_path = 'RAW/DATA/laboratory_data/P_GHB.xpt'\n",
    "P_GHB = pd.read_sas(P_GHB_path, format='xport', encoding = 'utf-8')\n",
    "\n",
    "base = P_GHB.copy()\n",
    "base['SEQN'] = pd.to_numeric(base['SEQN']).astype('Int64')\n",
    "base_idxed = base.set_index('SEQN')\n",
    "\n",
    "used_cols = set(base_idxed.columns) # Tracking used column names for collisions\n",
    "\n",
    "fileskip_list = [\n",
    "    # Base\n",
    "    'P_GHB.xpt',\n",
    "\n",
    "    # Dietary Data\n",
    "    'P_DR1IFF.xpt',\n",
    "    'P_DR2IFF.xpt',\n",
    "    'P_DR1TOT.xpt',\n",
    "    'P_DR2TOT.xpt',\n",
    "    'P_DRXFCD.xpt',\n",
    "    'DSBI.xpt',\n",
    "    'DSII.xpt',\n",
    "    'DSPI.xpt',\n",
    "    'P_DS1IDS.xpt',\n",
    "    'P_DS2IDS.xpt',\n",
    "    'P_DS1TOT.xpt',\n",
    "    'P_DS2TOT.xpt',\n",
    "    'P_DSQIDS.xpt',\n",
    "    'P_DSQTOT.xpt',\n",
    "\n",
    "    # Audiometry Sensor Data\n",
    "    'P_AUXAR.xpt',\n",
    "    'P_AUXTYM.xpt',\n",
    "    'P_AUXWBR.xpt',\n",
    "\n",
    "    # Medication Survey Data\n",
    "    'P_RXQ_RX.xpt'\n",
    "]\n",
    "\n",
    "dfs = []  # all other dfs, already indexed by SEQN\n",
    "log_path = Path(\"LOG/log_merge.txt\")\n",
    "\n",
    "with open(log_path, \"w\") as log:\n",
    "    for p in xpt_files:\n",
    "        filename = os.path.basename(p)\n",
    "        stem = Path(filename).stem\n",
    "        print(f\"Loading: {filename}\")\n",
    "\n",
    "        if filename in fileskip_list:\n",
    "            print(f\"\\tSkipping {filename} (manually excluded)\")\n",
    "            continue\n",
    "\n",
    "        # Read with fallback encoding\n",
    "        try:\n",
    "            df = pd.read_sas(p, format=\"xport\", encoding=\"utf-8\")\n",
    "        except UnicodeDecodeError:\n",
    "            df = pd.read_sas(p, format=\"xport\", encoding=\"cp1252\")\n",
    "\n",
    "        if 'SEQN' not in df.columns:\n",
    "            print(f\"\\tSkipping {filename}, missing SEQN\")\n",
    "            continue\n",
    "\n",
    "        \n",
    "        df['SEQN'] = pd.to_numeric(df['SEQN']).astype('Int64')\n",
    "\n",
    "        dup_counts = df['SEQN'].value_counts()\n",
    "        dup_counts = dup_counts[dup_counts > 1]\n",
    "        if not dup_counts.empty:\n",
    "            # print(f\"\\tSkipping {filename}. Found {dup_counts.size} duplicated SEQN values\")\n",
    "            msg = f\"Skipping {filename}. Found {dup_counts.size} duplicated SEQN values\\n\"\n",
    "            print(f\"\\t{msg.strip()}\")\n",
    "            log.write(msg)\n",
    "            continue\n",
    "        df = df.set_index('SEQN')\n",
    "        \n",
    "        # # Renaming only colliding columns (prefixing with filename)\n",
    "        # rename_mapper = {}\n",
    "        # for col in df.columns:\n",
    "        #     if col in used_cols:\n",
    "        #         rename_mapper[col] = f\"{filename}_{col}\"\n",
    "        # if rename_mapper:\n",
    "        #     df = df.rename(columns=rename_mapper)\n",
    "\n",
    "        df = df.rename(columns={col: f\"{stem}__{col}\" for col in df.columns})\n",
    "\n",
    "        # Check for duplicate columns across files\n",
    "        overlapping = [col for col in df.columns if col in used_cols]\n",
    "        if overlapping:\n",
    "            msg = f\"{filename}: Found {len(overlapping)} duplicate columns: {overlapping}\\n\"\n",
    "            print(f\"\\t{msg.strip()}\")\n",
    "            log.write(msg)\n",
    "\n",
    "        used_cols.update(df.columns) # Update used set\n",
    "\n",
    "        dfs.append(df)\n",
    "\n",
    "# ---- One big combine ----\n",
    "# Concatenate all other files horizontally (align on SEQN once)\n",
    "others = pd.concat(dfs, axis=1, copy=False)\n",
    "\n",
    "# Single join to the base\n",
    "merged = base_idxed.join(others, how='left')\n",
    "merged = merged.reset_index()\n",
    "\n",
    "print(f\"\\nMerging complete. Duplicate column log saved to {log_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351c19f8",
   "metadata": {},
   "source": [
    "# Merging Medicine Data from `P_RXQ_RX.xpt`\n",
    "\n",
    "The idea for this part is to include what kind of diabetes medication that the patient might take, which also might alter the a1c. This data can be used in two ways:\n",
    "* __Predicting Diabetes__  \n",
    "  This variable together with fasting glucose, a1c, diabetes questionnaire are used to determine the target variable, then dropped to ensure no leakage.  \n",
    "* __Predicting A1C (natural)__  \n",
    "  If the target is to understand the natural factor that affect a1c, then might drop the row for any treated patients.\n",
    "* __Predicting A1C (all factor)__  \n",
    "  Can be used as either as factor for more details, or make it as simple as treated vs untreated.\n",
    "\n",
    "**Algorithm**\n",
    "\n",
    "Step 1 - Import drug class table (manual work in excel)  \n",
    "Step 2 - Lookup and split the drug combination if any  \n",
    "Step 3 - From the drug list, binarize it  \n",
    "Step 4 - Summarize the binarized data to handle multiple records  \n",
    "Step 5 - Merge with the full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f917add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert XPT files to pandas dataframe\n",
    "def xpt_to_df(file_path):\n",
    "    df = pd.read_sas(file_path, format='xport', encoding='utf-8')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab41d6c",
   "metadata": {},
   "source": [
    "Step 1 - Import drug class table (manual work in excel)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ae3ff73",
   "metadata": {},
   "outputs": [],
   "source": [
    "P_RXQ_RX = xpt_to_df('INPUTS/UNPROCESSED/P_RXQ_RX.XPT')\n",
    "DIAMED_TAB = pd.read_excel(\"TABLES/DIAMED_TAB.xlsx\", sheet_name=\"DIAMED_TAB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331a5111",
   "metadata": {},
   "source": [
    "Step 2 - Lookup and split the drug combination if any  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ae80976",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_dict = dict(zip(DIAMED_TAB[\"DRUG_NAME\"].str.upper(), DIAMED_TAB[\"DRUG_CLASS\"]))\n",
    "\n",
    "def find_drug_class(drug_name):\n",
    "    if not isinstance(drug_name, str):\n",
    "        return []\n",
    "    parts = [x.strip().upper() for x in drug_name.split(';')]\n",
    "    return [drug_dict[p] for p in parts if p in drug_dict]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c66dcb",
   "metadata": {},
   "source": [
    "Step 3 - From the drug list, binarize it  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "155d29ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "P_RXQ_RX['drug_classes'] = P_RXQ_RX['RXDDRUG'].apply(find_drug_class)\n",
    "\n",
    "# One-hot encode the list of drug classes\n",
    "mlb = MultiLabelBinarizer()\n",
    "encoded = mlb.fit_transform(P_RXQ_RX['drug_classes'])\n",
    "encoded_df = pd.DataFrame(encoded, columns=mlb.classes_, index=P_RXQ_RX.index)\n",
    "\n",
    "# Combine with original DataFrame\n",
    "P_RXQ_RX = pd.concat([P_RXQ_RX, encoded_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575ade61",
   "metadata": {},
   "source": [
    "Step 4 - Summarize the binarized data to handle multiple records  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "648bc21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['SEQN'] + list(mlb.classes_)\n",
    "P_RXQ_RX_summary = P_RXQ_RX[cols].groupby(\"SEQN\", as_index=False).max() # groupby to summarize the SEQN, then max to indicate presence of drug class\n",
    "P_RXQ_RX_summary.to_csv(\"INPUTS/CSV/DIAMED.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7f8d4f",
   "metadata": {},
   "source": [
    "Step 5 - Merge with the full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83493158",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merged.merge(P_RXQ_RX_summary, on=\"SEQN\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400825ea",
   "metadata": {},
   "source": [
    "# Save merged to .parquet\n",
    "- Using parquet to keep data types\n",
    "- Requires `pyarrow` and `fastparquet`, listed now in requirements.txt. Make sure to restart kernel after installing to populate new pyarrow types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1873a30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved merged.parquet to ./PROCESSED/DATA/merged.parquet\n"
     ]
    }
   ],
   "source": [
    "directory = \"./PROCESSED/DATA\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "save_path = directory + \"/merged.parquet\"\n",
    "\n",
    "merged.to_parquet(save_path, index = False)\n",
    "print(f\"Saved merged.parquet to {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
