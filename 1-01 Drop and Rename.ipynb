{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bb3aa0e",
   "metadata": {},
   "source": [
    "# Merging, Drop, and Variable Renaming\n",
    "This is the initial pre-processing before doing the variable analysis.\n",
    "\n",
    "### Algorithm\n",
    "Step 1 - Merge the data  \n",
    "Step 2 - Remove the columns with more than 30% missingness  \n",
    "Step 3 - Create a dictionary  \n",
    "Step 4 - Lookup against dictionary  \n",
    "Step 5 - Split into test/train data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d04a62d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from pathlib import Path\n",
    "import os, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e50187eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert XPT files to pandas dataframe\n",
    "def xpt_to_df(file_path):\n",
    "    df = pd.read_sas(file_path, format='xport', encoding='utf-8')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ab02d4",
   "metadata": {},
   "source": [
    "Step 1 - Merge the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5d47020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: P_DEMO.xpt\n",
      "Loading: DSBI.xpt\n",
      "\tSkipping DSBI.xpt (manually excluded)\n",
      "Loading: DSII.xpt\n",
      "\tSkipping DSII.xpt (manually excluded)\n",
      "Loading: DSPI.xpt\n",
      "\tSkipping DSPI.xpt (manually excluded)\n",
      "Loading: P_DR1IFF.xpt\n",
      "\tSkipping P_DR1IFF.xpt (manually excluded)\n",
      "Loading: P_DR1TOT.xpt\n",
      "\tSkipping P_DR1TOT.xpt (manually excluded)\n",
      "Loading: P_DR2IFF.xpt\n",
      "\tSkipping P_DR2IFF.xpt (manually excluded)\n",
      "Loading: P_DR2TOT.xpt\n",
      "\tSkipping P_DR2TOT.xpt (manually excluded)\n",
      "Loading: P_DRXFCD.xpt\n",
      "\tSkipping P_DRXFCD.xpt (manually excluded)\n",
      "Loading: P_DS1IDS.xpt\n",
      "\tSkipping P_DS1IDS.xpt (manually excluded)\n",
      "Loading: P_DS1TOT.xpt\n",
      "\tSkipping P_DS1TOT.xpt (manually excluded)\n",
      "Loading: P_DS2IDS.xpt\n",
      "\tSkipping P_DS2IDS.xpt (manually excluded)\n",
      "Loading: P_DS2TOT.xpt\n",
      "\tSkipping P_DS2TOT.xpt (manually excluded)\n",
      "Loading: P_DSQIDS.xpt\n",
      "\tSkipping P_DSQIDS.xpt (manually excluded)\n",
      "Loading: P_DSQTOT.xpt\n",
      "\tSkipping P_DSQTOT.xpt (manually excluded)\n",
      "Loading: P_AUX.xpt\n",
      "Loading: P_AUXAR.xpt\n",
      "\tSkipping P_AUXAR.xpt (manually excluded)\n",
      "Loading: P_AUXTYM.xpt\n",
      "\tSkipping P_AUXTYM.xpt (manually excluded)\n",
      "Loading: P_AUXWBR.xpt\n",
      "\tSkipping P_AUXWBR.xpt (manually excluded)\n",
      "Loading: P_BMX.xpt\n",
      "Loading: P_BPXO.xpt\n",
      "Loading: P_DXXFEM.xpt\n",
      "Loading: P_DXXSPN.xpt\n",
      "Loading: P_LUX.xpt\n",
      "Loading: P_OHXDEN.xpt\n",
      "Loading: P_OHXREF.xpt\n",
      "Loading: P_ALB_CR.xpt\n",
      "Loading: P_BIOPRO.xpt\n",
      "Loading: P_CBC.xpt\n",
      "Loading: P_CMV.xpt\n",
      "Loading: P_COT.xpt\n",
      "Loading: P_CRCO.xpt\n",
      "Loading: P_ETHOX.xpt\n",
      "Loading: P_FASTQX.xpt\n",
      "Loading: P_FERTIN.xpt\n",
      "Loading: P_FETIB.xpt\n",
      "Loading: P_FOLATE.xpt\n",
      "Loading: P_FOLFMS.xpt\n",
      "Loading: P_FR.xpt\n",
      "Loading: P_GHB.xpt\n",
      "Loading: P_GLU.xpt\n",
      "Loading: P_HDL.xpt\n",
      "Loading: P_HEPA.xpt\n",
      "Loading: P_HEPBD.xpt\n",
      "Loading: P_HEPB_S.xpt\n",
      "Loading: P_HEPC.xpt\n",
      "Loading: P_HEPE.xpt\n",
      "Loading: P_HSCRP.xpt\n",
      "Loading: P_IHGEM.xpt\n",
      "Loading: P_INS.xpt\n",
      "Loading: P_OPD.xpt\n",
      "Loading: P_PBCD.xpt\n",
      "Loading: P_PERNT.xpt\n",
      "Loading: P_PFAS.xpt\n",
      "Loading: P_SSAGP.xpt\n",
      "Loading: P_SSFR.xpt\n",
      "Loading: P_TCHOL.xpt\n",
      "Loading: P_TFR.xpt\n",
      "Loading: P_TRIGLY.xpt\n",
      "Loading: P_TST.xpt\n",
      "Loading: P_UAS.xpt\n",
      "Loading: P_UCFLOW.xpt\n",
      "Loading: P_UCM.xpt\n",
      "Loading: P_UCPREG.xpt\n",
      "Loading: P_UHG.xpt\n",
      "Loading: P_UIO.xpt\n",
      "Loading: P_UM.xpt\n",
      "Loading: P_UNI.xpt\n",
      "Loading: P_UTAS.xpt\n",
      "Loading: P_UVOC.xpt\n",
      "Loading: P_UVOC2.xpt\n",
      "Loading: P_VOCWB.xpt\n",
      "Loading: P_ACQ.xpt\n",
      "Loading: P_ALQ.xpt\n",
      "Loading: P_AUQ.xpt\n",
      "Loading: P_BPQ.xpt\n",
      "Loading: P_CBQPFA.xpt\n",
      "Loading: P_CBQPFC.xpt\n",
      "Loading: P_CDQ.xpt\n",
      "Loading: P_DBQ.xpt\n",
      "Loading: P_DEQ.xpt\n",
      "Loading: P_DIQ.xpt\n",
      "Loading: P_DPQ.xpt\n",
      "Loading: P_ECQ.xpt\n",
      "Loading: P_FSQ.xpt\n",
      "Loading: P_HEQ.xpt\n",
      "Loading: P_HIQ.xpt\n",
      "Loading: P_HSQ.xpt\n",
      "Loading: P_HUQ.xpt\n",
      "Loading: P_IMQ.xpt\n",
      "Loading: P_INQ.xpt\n",
      "Loading: P_KIQ_U.xpt\n",
      "Loading: P_MCQ.xpt\n",
      "Loading: P_OCQ.xpt\n",
      "Loading: P_OHQ.xpt\n",
      "Loading: P_OSQ.xpt\n",
      "Loading: P_PAQ.xpt\n",
      "Loading: P_PAQY.xpt\n",
      "Loading: P_PUQMEC.xpt\n",
      "Loading: P_RHQ.xpt\n",
      "Loading: P_RXQASA.xpt\n",
      "Loading: P_RXQ_RX.xpt\n",
      "\tSkipping P_RXQ_RX.xpt (manually excluded)\n",
      "Loading: P_SLQ.xpt\n",
      "Loading: P_SMQ.xpt\n",
      "Loading: P_SMQFAM.xpt\n",
      "Loading: P_SMQRTU.xpt\n",
      "Loading: P_SMQSHS.xpt\n",
      "Loading: P_VTQ.xpt\n",
      "Loading: P_WHQ.xpt\n",
      "Loading: P_WHQMEC.xpt\n",
      "Loading: RXQ_DRUG.xpt\n",
      "\tSkipping RXQ_DRUG.xpt, missing SEQN\n",
      "\n",
      "Merging complete. Duplicate column log saved to C:\\Users\\victo\\Documents\\CSE 881\\PROJECT\\CSE881_PROJ\\LOG\\log_merge.txt\n"
     ]
    }
   ],
   "source": [
    "root = Path(\"RAW/DATA\")\n",
    "xpt_files = list(root.rglob(\"*.xpt\"))\n",
    "\n",
    "P_GHB_path = 'RAW/DATA/laboratory_data/P_GHB.xpt'\n",
    "P_GHB = pd.read_sas(P_GHB_path, format='xport', encoding = 'utf-8')\n",
    "\n",
    "base = P_GHB.copy()\n",
    "base['SEQN'] = pd.to_numeric(base['SEQN']).astype('Int64')\n",
    "base_idxed = base.set_index('SEQN')\n",
    "\n",
    "used_cols = set(base_idxed.columns) # Tracking used column names for collisions\n",
    "\n",
    "fileskip_list = [\n",
    "    # Dietary Data\n",
    "    'P_DR1IFF.xpt',\n",
    "    'P_DR2IFF.xpt',\n",
    "    'P_DR1TOT.xpt',\n",
    "    'P_DR2TOT.xpt',\n",
    "    'P_DRXFCD.xpt',\n",
    "    'DSBI.xpt',\n",
    "    'DSII.xpt',\n",
    "    'DSPI.xpt',\n",
    "    'P_DS1IDS.xpt',\n",
    "    'P_DS2IDS.xpt',\n",
    "    'P_DS1TOT.xpt',\n",
    "    'P_DS2TOT.xpt',\n",
    "    'P_DSQIDS.xpt',\n",
    "    'P_DSQTOT.xpt',\n",
    "\n",
    "    # Audiometry Sensor Data\n",
    "    'P_AUXAR.xpt',\n",
    "    'P_AUXTYM.xpt',\n",
    "    'P_AUXWBR.xpt',\n",
    "\n",
    "    # Medication Survey Data\n",
    "    'P_RXQ_RX.xpt'\n",
    "]\n",
    "\n",
    "dfs = []  # all other dfs, already indexed by SEQN\n",
    "log_path = Path(\"LOG/log_merge.txt\")\n",
    "\n",
    "with open(log_path, \"w\") as log:\n",
    "    for p in xpt_files:\n",
    "        filename = os.path.basename(p)\n",
    "        stem = Path(filename).stem\n",
    "        print(f\"Loading: {filename}\")\n",
    "\n",
    "        if filename in fileskip_list:\n",
    "            print(f\"\\tSkipping {filename} (manually excluded)\")\n",
    "            continue\n",
    "\n",
    "        # Read with fallback encoding\n",
    "        try:\n",
    "            df = pd.read_sas(p, format=\"xport\", encoding=\"utf-8\")\n",
    "        except UnicodeDecodeError:\n",
    "            df = pd.read_sas(p, format=\"xport\", encoding=\"cp1252\")\n",
    "\n",
    "        if 'SEQN' not in df.columns:\n",
    "            print(f\"\\tSkipping {filename}, missing SEQN\")\n",
    "            continue\n",
    "\n",
    "        \n",
    "        df['SEQN'] = pd.to_numeric(df['SEQN']).astype('Int64')\n",
    "\n",
    "        dup_counts = df['SEQN'].value_counts()\n",
    "        dup_counts = dup_counts[dup_counts > 1]\n",
    "        if not dup_counts.empty:\n",
    "            # print(f\"\\tSkipping {filename}. Found {dup_counts.size} duplicated SEQN values\")\n",
    "            msg = f\"Skipping {filename}. Found {dup_counts.size} duplicated SEQN values\\n\"\n",
    "            print(f\"\\t{msg.strip()}\")\n",
    "            log.write(msg)\n",
    "            continue\n",
    "        df = df.set_index('SEQN')\n",
    "        \n",
    "        # # Renaming only colliding columns (prefixing with filename)\n",
    "        # rename_mapper = {}\n",
    "        # for col in df.columns:\n",
    "        #     if col in used_cols:\n",
    "        #         rename_mapper[col] = f\"{filename}_{col}\"\n",
    "        # if rename_mapper:\n",
    "        #     df = df.rename(columns=rename_mapper)\n",
    "\n",
    "        df = df.rename(columns={col: f\"{stem}__{col}\" for col in df.columns})\n",
    "\n",
    "        # Check for duplicate columns across files\n",
    "        overlapping = [col for col in df.columns if col in used_cols]\n",
    "        if overlapping:\n",
    "            msg = f\"{filename}: Found {len(overlapping)} duplicate columns: {overlapping}\\n\"\n",
    "            print(f\"\\t{msg.strip()}\")\n",
    "            log.write(msg)\n",
    "\n",
    "        used_cols.update(df.columns) # Update used set\n",
    "\n",
    "        dfs.append(df)\n",
    "\n",
    "# ---- One big combine ----\n",
    "# Concatenate all other files horizontally (align on SEQN once)\n",
    "others = pd.concat(dfs, axis=1, copy=False)\n",
    "\n",
    "# Single join to the base\n",
    "df_01_merged = base_idxed.join(others, how='left')\n",
    "df_01_merged = df_01_merged.reset_index()\n",
    "\n",
    "print(f\"\\nMerging complete. Duplicate column log saved to {log_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5170c9b3",
   "metadata": {},
   "source": [
    "Step 2 - Remove the columns with more than 30% missingness  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dd72e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 1107 columns with >30% missingness.\n",
      "Dropped 672 rows with missing target variable (P_GHB__LBXGH).\n",
      "Cleaning log saved to C:\\Users\\victo\\Documents\\CSE 881\\PROJECT\\CSE881_PROJ\\LOG\\log_cleaning.txt\n"
     ]
    }
   ],
   "source": [
    "# Step 2a - Remove columns with more than 30% missingness\n",
    "num_rows = df_01_merged.shape[0]\n",
    "threshold = 0.3 * num_rows\n",
    "\n",
    "cols_to_drop = df_01_merged.columns[df_01_merged.isnull().sum() > threshold]\n",
    "df_02_dropped = df_01_merged.drop(columns=cols_to_drop)\n",
    "\n",
    "# Step 2b - Remove rows with no target variable\n",
    "target_col = 'P_GHB__LBXGH'\n",
    "before_rows = df_02_dropped.shape[0]\n",
    "df_02_dropped = df_02_dropped.dropna(subset=[target_col])\n",
    "after_rows = df_02_dropped.shape[0]\n",
    "rows_dropped = before_rows - after_rows\n",
    "\n",
    "# Logging the cleaning steps\n",
    "log_path = Path(\"LOG/log_cleaning.txt\")\n",
    "with open(log_path, \"w\") as log:\n",
    "    log.write(\"==== Data Cleaning Log ====\\n\\n\")\n",
    "\n",
    "    # Log columns dropped\n",
    "    log.write(f\"Columns dropped (>30% missingness): {len(cols_to_drop)}\\n\")\n",
    "    if len(cols_to_drop) > 0:\n",
    "        log.write(\"\\n\".join(cols_to_drop))\n",
    "        log.write(\"\\n\\n\")\n",
    "    else:\n",
    "        log.write(\"None\\n\\n\")\n",
    "\n",
    "    # Log rows dropped\n",
    "    log.write(f\"Rows dropped with missing target variable ({target_col}): {rows_dropped}\\n\")\n",
    "\n",
    "print(f\"Dropped {len(cols_to_drop)} columns with >30% missingness.\")\n",
    "print(f\"Dropped {rows_dropped} rows with missing target variable ({target_col}).\")\n",
    "print(f\"Cleaning log saved to {log_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8336cbf",
   "metadata": {},
   "source": [
    "Step 3 - Create a dictionary  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fed555a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved dictionary to: C:\\Users\\victo\\Documents\\CSE 881\\PROJECT\\CSE881_PROJ\\TABLES\\dictionary.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# Folder with all codebooks\n",
    "codebook_root = Path(\"RAW/CODEBOOKS\")\n",
    "\n",
    "# Helper to clean symbols\n",
    "def clean_text(s):\n",
    "    s = \"\" if pd.isna(s) else str(s)\n",
    "    s = re.sub(r\"[^A-Za-z0-9_]+\", \"_\", s)\n",
    "    return re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "\n",
    "# Collect all codebook files\n",
    "merged = []\n",
    "for f in codebook_root.rglob(\"*_codebook.csv\"):\n",
    "    df = pd.read_csv(f)\n",
    "    df.columns = [c.lower().replace(\" \", \"_\") for c in df.columns]\n",
    "    df = df.rename(columns={\"variable_name\": \"var\", \"variable\": \"var\",\n",
    "                            \"sas_label\": \"label\", \"label\": \"label\"})\n",
    "    if \"var\" not in df or \"label\" not in df:\n",
    "        continue\n",
    "    data_file = Path(f).stem.replace(\"_codebook\", \"\")\n",
    "    df[\"variable_name\"] = df[\"var\"].apply(clean_text)\n",
    "    df[\"sas_label\"] = df[\"label\"].apply(clean_text)\n",
    "    df[\"variable_label\"] = df[\"variable_name\"] + \"_\" + df[\"sas_label\"]\n",
    "    df[\"data_file\"] = data_file\n",
    "    merged.append(df[[\"data_file\", \"variable_name\", \"sas_label\", \"variable_label\"]])\n",
    "\n",
    "# Combine and save\n",
    "if merged:\n",
    "    out = pd.concat(merged, ignore_index=True)\n",
    "    output_path = Path(\"TABLES/dictionary.csv\")\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    out.to_csv(output_path, index=False)\n",
    "    print(f\"Saved dictionary to: {output_path.resolve()}\")\n",
    "else:\n",
    "    print(\"No valid codebooks found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704ee618",
   "metadata": {},
   "source": [
    "Step 4 - Lookup against dictionary  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ea592d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renamed 3738 columns using dictionary.\n",
      "Saved renamed dataframe to: C:\\Users\\victo\\Documents\\CSE 881\\PROJECT\\CSE881_PROJ\\INPUTS\\CSV\\df_03_renamed.csv\n"
     ]
    }
   ],
   "source": [
    "dict_path = \"TABLES/dictionary.csv\"\n",
    "df_dict = pd.read_csv(dict_path)\n",
    "\n",
    "# Mapping: {old_name: new_name}\n",
    "# The old name is data_file + \"__\" + variable_name (same as in merged data)\n",
    "df_dict[\"old_name\"] = df_dict[\"data_file\"] + \"__\" + df_dict[\"variable_name\"]\n",
    "rename_map = dict(zip(df_dict[\"old_name\"], df_dict[\"variable_label\"]))\n",
    "\n",
    "# Apply renaming\n",
    "df_03_renamed = df_02_dropped.rename(columns=rename_map)\n",
    "\n",
    "print(f\"Renamed {len(rename_map)} columns using dictionary.\")\n",
    "output_path = Path(\"INPUTS/CSV/df_03_renamed.csv\")\n",
    "df_03_renamed.to_csv(output_path, index=False)\n",
    "print(f\"Saved renamed dataframe to: {output_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a7b0f4",
   "metadata": {},
   "source": [
    "Step 5 - Flag and set table output to clearly categorical data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f37e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved variable flags to: C:\\Users\\victo\\Documents\\CSE 881\\PROJECT\\CSE881_PROJ\\TABLES\\variable_flags.csv\n",
      "IS_CATEGORICAL\n",
      "         501\n",
      "P_MCQ     20\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = df_03_renamed.copy()\n",
    "\n",
    "flags = []\n",
    "dup_counter = {}  # to label duplicate column names in the output\n",
    "\n",
    "for i, col in enumerate(df.columns):\n",
    "    # Make an output-safe name for duplicates\n",
    "    k = dup_counter.get(col, 0)\n",
    "    out_name = col if k == 0 else f\"{col}__dup{k}\"\n",
    "    dup_counter[col] = k + 1\n",
    "\n",
    "    # Always select by index -> guaranteed 1-D Series\n",
    "    s = df.iloc[:, i].dropna()\n",
    "\n",
    "    # Empty -> treat as numeric (False) to avoid all-True\n",
    "    if s.empty:\n",
    "        flags.append((out_name, False))\n",
    "        continue\n",
    "\n",
    "    # Try convert to numeric on the *values* only\n",
    "    s_num = pd.to_numeric(pd.Series(s.values), errors=\"coerce\")\n",
    "    numeric_ratio = s_num.notna().mean()\n",
    "\n",
    "    # Mostly non-numeric -> categorical\n",
    "    if numeric_ratio < 0.9:\n",
    "        flags.append((out_name, True))\n",
    "        continue\n",
    "\n",
    "    # Numeric path\n",
    "    s_num = s_num.dropna()\n",
    "\n",
    "    # Any decimals -> numeric\n",
    "    if (s_num % 1 != 0).any():\n",
    "        flags.append((out_name, False))\n",
    "        continue\n",
    "\n",
    "    # Integer-like -> categorical if few codes\n",
    "    nunique = s_num.nunique()\n",
    "    if nunique <= 20 or (nunique / len(s_num)) < 0.01:\n",
    "        flags.append((out_name, True))\n",
    "    else:\n",
    "        flags.append((out_name, False))\n",
    "\n",
    "# Build and save flag table\n",
    "df_flag = pd.DataFrame(flags, columns=[\"var_name\", \"IS_CATEGORICAL\"])\n",
    "out_path = Path(\"TABLES/variable_flags.csv\")\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "df_flag.to_csv(out_path, index=False)\n",
    "\n",
    "print(f\"Saved variable flags to: {out_path.resolve()}\")\n",
    "print(df_flag[\"IS_CATEGORICAL\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d8d9c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved variable flags to: C:\\Users\\victo\\Documents\\CSE 881\\PROJECT\\CSE881_PROJ\\TABLES\\variable_flags.csv\n",
      "IS_CATEGORICAL\n",
      "True     326\n",
      "False    195\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Step 5 – Flag categorical vs numeric and include file_name (from dictionary)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "df = df_03_renamed.copy()\n",
    "\n",
    "# Load dictionary: must contain columns [\"variable_label\",\"data_file\"]\n",
    "dict_path = Path(\"TABLES/dictionary.csv\")\n",
    "df_dict = pd.read_csv(dict_path)\n",
    "\n",
    "# Map each final column name (variable_label) -> data_file\n",
    "label_to_file = dict(zip(df_dict[\"variable_label\"], df_dict[\"data_file\"]))\n",
    "\n",
    "flags = []\n",
    "dup_counter = {}\n",
    "\n",
    "for i, col in enumerate(df.columns):\n",
    "    # Unique output name for duplicates\n",
    "    k = dup_counter.get(col, 0)\n",
    "    out_name = col if k == 0 else f\"{col}__dup{k}\"\n",
    "    dup_counter[col] = k + 1\n",
    "\n",
    "    # Lookup file_name from dictionary (blank if not found)\n",
    "    file_name = label_to_file.get(col, \"\")\n",
    "\n",
    "    # Always select by column index to ensure 1-D Series\n",
    "    s = df.iloc[:, i].dropna()\n",
    "\n",
    "    if s.empty:\n",
    "        flags.append((out_name, file_name, False))\n",
    "        continue\n",
    "\n",
    "    # Try numeric conversion\n",
    "    s_num = pd.to_numeric(pd.Series(s.values), errors=\"coerce\")\n",
    "    numeric_ratio = s_num.notna().mean()\n",
    "\n",
    "    # Mostly non-numeric -> categorical\n",
    "    if numeric_ratio < 0.9:\n",
    "        flags.append((out_name, file_name, True))\n",
    "        continue\n",
    "\n",
    "    s_num = s_num.dropna()\n",
    "\n",
    "    # Any decimals -> numeric\n",
    "    if (s_num % 1 != 0).any():\n",
    "        flags.append((out_name, file_name, False))\n",
    "        continue\n",
    "\n",
    "    # Integer-like -> categorical if few codes\n",
    "    nunique = s_num.nunique()\n",
    "    if nunique <= 20 or (nunique / len(s_num)) < 0.01:\n",
    "        flags.append((out_name, file_name, True))\n",
    "    else:\n",
    "        flags.append((out_name, file_name, False))\n",
    "\n",
    "# Save result\n",
    "df_flag = pd.DataFrame(flags, columns=[\"var_name\", \"file_name\", \"IS_CATEGORICAL\"])\n",
    "out_path = Path(\"TABLES/variable_flags.csv\")\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "df_flag.to_csv(out_path, index=False)\n",
    "\n",
    "print(f\"Saved variable flags to: {out_path.resolve()}\")\n",
    "print(df_flag[\"IS_CATEGORICAL\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a14ec9",
   "metadata": {},
   "source": [
    "Step 6 - Split into test/train data  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
