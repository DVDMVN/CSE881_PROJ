{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53a0c941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from tabnanny import verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "555b000d",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"./PROCESSED/DATA\"\n",
    "file_name = \"merged_and_dropped.parquet\"\n",
    "path = f\"{directory}/{file_name}\"\n",
    "\n",
    "df = pd.read_parquet(path)\n",
    "\n",
    "with open(\"./PROCESSED/DATA/merged_and_dropped.cat_cols.json\") as f:\n",
    "    cat_cols = json.load(f)\n",
    "\n",
    "df[cat_cols] = df[cat_cols].astype(\"category\")\n",
    "\n",
    "# drop SEQN\n",
    "df = df.drop(columns=['SEQN'])\n",
    "\n",
    "dtypes = df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2600cd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train-test\n",
    "train_pre_cleaned, test_pre_cleaned = train_test_split(df, test_size=0.2, random_state=42)\n",
    "X = df.drop(columns=['LBXGH'])\n",
    "y = df[['LBXGH']]\n",
    "X_train_pre_cleaned, X_test_pre_cleaned, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "pd.DataFrame(X_test_pre_cleaned).to_csv(\"RESULTS/X_test_pre_cleaned.csv\", index=False) # for manual checking purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073d3759",
   "metadata": {},
   "source": [
    "# Imputation\n",
    "Random Forest imputation using sklearn iterativeimputer. MissForest doesn't allow to re-use the model to impute the test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25ebac7",
   "metadata": {},
   "source": [
    "step 1 - encode both train/test   \n",
    "step 2 - fit iterativeimputer random forest on train set only  \n",
    "step 3 - use trained imputation model on both train and test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02370dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data is encoded to ordinal encoder since iterative imputer only accepts numerical values\n",
    "# Trees can handle ordinal encoded categorical variables without issue\n",
    "# Using one-hot encoding would increase the number of features too much\n",
    "# However, linear models would require one-hot encoding to avoid implying ordinality\n",
    "# And also for distribution tests later, we need to decode back to original categories\n",
    "\n",
    "# named cat_cols instead of cat_vars to avoid confusion\n",
    "cat_cols = X_train_pre_cleaned.select_dtypes(include=['category']).columns.tolist()\n",
    "# cat_cols = X_train_pre_cleaned.select_dtypes(include=['category', 'object']).columns.tolist() # include 'object' dtype as well\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "X_train_ordinal = X_train_pre_cleaned.copy()\n",
    "X_test_ordinal = X_test_pre_cleaned.copy()\n",
    "X_train_ordinal[cat_cols] = ordinal_encoder.fit_transform(X_train_ordinal[cat_cols])\n",
    "X_test_ordinal[cat_cols] = ordinal_encoder.transform(X_test_ordinal[cat_cols])\n",
    "\n",
    "# transform \"unknown\" into NaN so the imputer imputes them\n",
    "for c in cat_cols:\n",
    "    X_train_ordinal[c] = X_train_ordinal[c].replace(-1, np.nan)\n",
    "    X_test_ordinal[c] = X_test_ordinal[c].replace(-1, np.nan)\n",
    "\n",
    "# not needed but in case want to try distance-based imputers later\n",
    "# num_cols = [c for c in X_train_ordinal.columns if c not in cat_cols]\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = X_train_ordinal.copy()\n",
    "# X_test_scaled = X_test_ordinal.copy()\n",
    "# X_train_scaled[num_cols] = scaler.fit_transform(X_train_scaled[num_cols])\n",
    "# X_test_scaled[num_cols] = scaler.transform(X_test_scaled[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af7da6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IterativeImputer] Completing matrix with shape (7789, 258)\n",
      "[IterativeImputer] Ending imputation round 1/10, elapsed time 53.83\n",
      "[IterativeImputer] Change: 10899.125801659964, scaled tolerance: 68.0 \n",
      "[IterativeImputer] Ending imputation round 2/10, elapsed time 111.18\n",
      "[IterativeImputer] Change: 3336.059715225099, scaled tolerance: 68.0 \n",
      "[IterativeImputer] Ending imputation round 3/10, elapsed time 173.39\n",
      "[IterativeImputer] Change: 3149.926291026937, scaled tolerance: 68.0 \n",
      "[IterativeImputer] Ending imputation round 4/10, elapsed time 240.26\n",
      "[IterativeImputer] Change: 4149.978610087231, scaled tolerance: 68.0 \n",
      "[IterativeImputer] Ending imputation round 5/10, elapsed time 315.10\n",
      "[IterativeImputer] Change: 3528.136456203069, scaled tolerance: 68.0 \n",
      "[IterativeImputer] Ending imputation round 6/10, elapsed time 392.34\n",
      "[IterativeImputer] Change: 2937.1316447911468, scaled tolerance: 68.0 \n",
      "[IterativeImputer] Ending imputation round 7/10, elapsed time 465.02\n",
      "[IterativeImputer] Change: 3353.4781028894085, scaled tolerance: 68.0 \n",
      "[IterativeImputer] Ending imputation round 8/10, elapsed time 544.64\n",
      "[IterativeImputer] Change: 3164.169449930366, scaled tolerance: 68.0 \n",
      "[IterativeImputer] Ending imputation round 9/10, elapsed time 629.46\n",
      "[IterativeImputer] Change: 2622.546317037115, scaled tolerance: 68.0 \n",
      "[IterativeImputer] Ending imputation round 10/10, elapsed time 729.57\n",
      "[IterativeImputer] Change: 3090.704479785133, scaled tolerance: 68.0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\victo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\impute\\_iterative.py:825: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# For a random forest imputer, we don't really need a huge number of trees\n",
    "# Imputation is about generating stable estimates, not prediction accuracy\n",
    "# So we can limit the number of trees to speed up computation\n",
    "# Usually n=10-50 is sufficient, but we can go a bit higher if it's unstable\n",
    "\n",
    "# def random_forest_imputer(n_estimators=20, max_iter=3):\n",
    "#     rf_imputer = IterativeImputer(\n",
    "#         estimator=RandomForestRegressor(n_estimators=n_estimators, max_depth=10, n_jobs=-1, random_state=42),\n",
    "#         max_iter=max_iter,\n",
    "#         random_state=42\n",
    "#     )\n",
    "#     return rf_imputer\n",
    "\n",
    "\n",
    "# def random_forest_imputer():\n",
    "#     rf_imputer = IterativeImputer(\n",
    "#         estimator=RandomForestRegressor(\n",
    "#             n_estimators=20,\n",
    "#             max_depth=10,\n",
    "#             min_samples_leaf=30,     # increased for stability\n",
    "#             max_features='sqrt',\n",
    "#             n_jobs=-1,\n",
    "#             random_state=42\n",
    "#         ),\n",
    "#         n_nearest_features=60,       # uses most relevant subset of features\n",
    "#         max_iter=10,\n",
    "#         tol=0.01,                    # balanced tolerance for convergence\n",
    "#         initial_strategy='median',\n",
    "#         random_state=42,\n",
    "#         verbose=2\n",
    "#     )\n",
    "#     return rf_imputer\n",
    "\n",
    "# def random_forest_imputer():\n",
    "#     rf_imputer = IterativeImputer(\n",
    "#         estimator=RandomForestRegressor(\n",
    "#             n_estimators=20,\n",
    "#             max_depth=15,\n",
    "#             min_samples_leaf=20,\n",
    "#             min_samples_split=40,\n",
    "#             max_features='sqrt',\n",
    "#             n_jobs=-1,\n",
    "#             random_state=42\n",
    "#         ),\n",
    "#         n_nearest_features=60,\n",
    "#         max_iter=20,\n",
    "#         tol=0.01,\n",
    "#         initial_strategy='median',\n",
    "#         random_state=42,\n",
    "#         verbose=2\n",
    "#     )\n",
    "#     return rf_imputer\n",
    "\n",
    "# def extra_trees_imputer(n_estimators=20, max_iter=3, verbose=0):\n",
    "#     et_imputer = IterativeImputer(\n",
    "#         estimator=ExtraTreesRegressor(n_estimators=n_estimators, max_depth=10, n_jobs=-1, random_state=42),\n",
    "#         max_iter=max_iter,\n",
    "#         random_state=42,\n",
    "#         verbose=verbose\n",
    "#     )\n",
    "#     return et_imputer\n",
    "\n",
    "# def extra_trees_imputer(n_estimators=20, max_iter=10, verbose=0):\n",
    "#     et_imputer = IterativeImputer(\n",
    "#         estimator=ExtraTreesRegressor(\n",
    "#             n_estimators=n_estimators, \n",
    "#             max_depth=6, \n",
    "#             min_samples_leaf=10,\n",
    "#             n_jobs=-1, \n",
    "#             random_state=42\n",
    "#         ),\n",
    "#         max_iter=max_iter,\n",
    "#         initial_strategy='median',\n",
    "#         random_state=42,\n",
    "#         verbose=verbose\n",
    "#     )\n",
    "#     return et_imputer\n",
    "\n",
    "# def extra_trees_imputer():\n",
    "#     et_imputer = IterativeImputer(\n",
    "#         estimator=ExtraTreesRegressor(\n",
    "#             n_estimators=30,\n",
    "#             max_depth=10,\n",
    "#             min_samples_leaf=30,     # increased for stability\n",
    "#             max_features='sqrt',\n",
    "#             n_jobs=-1,\n",
    "#             random_state=42\n",
    "#         ),\n",
    "#         n_nearest_features=60,       # uses most relevant subset of features\n",
    "#         max_iter=10,\n",
    "#         tol=0.01,                    # 0.001 is too small\n",
    "#         initial_strategy='median',\n",
    "#         random_state=42,\n",
    "#         verbose=2\n",
    "#     )\n",
    "#     return et_imputer\n",
    "\n",
    "\n",
    "def extra_trees_imputer():\n",
    "    et_imputer = IterativeImputer(\n",
    "        estimator=ExtraTreesRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=12,\n",
    "            min_samples_leaf=30,\n",
    "            min_samples_split=40,\n",
    "            max_features='sqrt',\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        ),\n",
    "        n_nearest_features=30,\n",
    "        max_iter=10,\n",
    "        initial_strategy='median',\n",
    "        random_state=42,\n",
    "        imputation_order='random',\n",
    "        verbose=2\n",
    "    )\n",
    "    return et_imputer\n",
    "\n",
    "\n",
    "\n",
    "imputer = extra_trees_imputer()\n",
    "# imputer = random_forest_imputer()\n",
    "\n",
    "X_train = imputer.fit_transform(X_train_ordinal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fd1616",
   "metadata": {},
   "source": [
    "> The iterative imputer did not reach the convergence tolerance. However, the change is stabilized and non-divergent after first few iterations, which suffice for practical purpose. The imputation was terminated after 10 rounds as further iteration unlikely will produce any meaningful improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bfd6d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(imputer, \"RESULTS/extra_trees_imputer.pkl\", compress=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c038853f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IterativeImputer] Completing matrix with shape (1948, 258)\n",
      "[IterativeImputer] Ending imputation round 1/10, elapsed time 9.56\n",
      "[IterativeImputer] Ending imputation round 2/10, elapsed time 19.10\n",
      "[IterativeImputer] Ending imputation round 3/10, elapsed time 28.93\n",
      "[IterativeImputer] Ending imputation round 4/10, elapsed time 38.13\n",
      "[IterativeImputer] Ending imputation round 5/10, elapsed time 47.38\n",
      "[IterativeImputer] Ending imputation round 6/10, elapsed time 56.70\n",
      "[IterativeImputer] Ending imputation round 7/10, elapsed time 66.01\n",
      "[IterativeImputer] Ending imputation round 8/10, elapsed time 74.58\n",
      "[IterativeImputer] Ending imputation round 9/10, elapsed time 82.88\n",
      "[IterativeImputer] Ending imputation round 10/10, elapsed time 90.93\n"
     ]
    }
   ],
   "source": [
    "X_test = imputer.transform(X_test_ordinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7521b2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert back to DataFrames\n",
    "X_train_imputed = pd.DataFrame(X_train, columns=X_train_ordinal.columns, index=X_train_ordinal.index)\n",
    "X_test_imputed  = pd.DataFrame(X_test, columns=X_test_ordinal.columns,  index=X_test_ordinal.index)\n",
    "\n",
    "# reverse scaling for numeric columns, if applied\n",
    "# X_train_imputed[num_cols] = scaler.inverse_transform(X_train_imputed[num_cols])\n",
    "# X_test_imputed[num_cols] = scaler.inverse_transform(X_test_imputed[num_cols])\n",
    "\n",
    "# rounding categorical codes to valid range before inverse_transform\n",
    "for i, c in enumerate(cat_cols):\n",
    "    n = len(ordinal_encoder.categories_[i])\n",
    "    X_train_imputed[c] = np.clip(np.rint(X_train_imputed[c]).astype(int), 0, n-1)\n",
    "    X_test_imputed[c] = np.clip(np.rint(X_test_imputed[c]).astype(int),  0, n-1)\n",
    "\n",
    "# restore categorical dtype\n",
    "X_train_imputed[cat_cols] = ordinal_encoder.inverse_transform(X_train_imputed[cat_cols])\n",
    "X_test_imputed[cat_cols] = ordinal_encoder.inverse_transform(X_test_imputed[cat_cols])\n",
    "for col in cat_cols:\n",
    "    X_train_imputed[col] = X_train_imputed[col].astype('category')\n",
    "    X_test_imputed[col] = X_test_imputed[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57191e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"INPUTS/TRAIN\", exist_ok=True)\n",
    "os.makedirs(\"INPUTS/TEST\", exist_ok=True)\n",
    "os.makedirs(\"RESULTS\", exist_ok=True)\n",
    "\n",
    "X_train_imputed.to_parquet(\"INPUTS/TRAIN/X_train.parquet\", index=False)\n",
    "X_test_imputed.to_parquet(\"INPUTS/TEST/X_test.parquet\", index=False)\n",
    "y_train.to_parquet(\"INPUTS/TRAIN/y_train.parquet\", index=False)\n",
    "y_test.to_parquet(\"INPUTS/TEST/y_test.parquet\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
