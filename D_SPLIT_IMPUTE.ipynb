{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53a0c941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from tabnanny import verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "555b000d",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"./PROCESSED/DATA\"\n",
    "file_name = \"merged_and_dropped.parquet\"\n",
    "path = f\"{directory}/{file_name}\"\n",
    "\n",
    "df = pd.read_parquet(path)\n",
    "\n",
    "with open(\"./PROCESSED/DATA/merged_and_dropped.cat_cols.json\") as f:\n",
    "    cat_cols = json.load(f)\n",
    "\n",
    "df[cat_cols] = df[cat_cols].astype(\"category\")\n",
    "\n",
    "# drop SEQN\n",
    "df = df.drop(columns=['SEQN'])\n",
    "\n",
    "dtypes = df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2600cd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train-test\n",
    "train_pre_cleaned, test_pre_cleaned = train_test_split(df, test_size=0.2, random_state=42)\n",
    "X = df.drop(columns=['LBXGH'])\n",
    "y = df[['LBXGH']]\n",
    "X_train_pre_cleaned, X_test_pre_cleaned, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "pd.DataFrame(X_test_pre_cleaned).to_csv(\"RESULTS/X_test_pre_cleaned.csv\", index=False) # for manual checking purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073d3759",
   "metadata": {},
   "source": [
    "# Imputation\n",
    "Random Forest imputation using sklearn iterativeimputer. MissForest doesn't allow to re-use the model to impute the test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25ebac7",
   "metadata": {},
   "source": [
    "step 1 - encode both train/test   \n",
    "step 2 - fit iterativeimputer random forest on train set only  \n",
    "step 3 - use trained imputation model on both train and test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a02370dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data is encoded to ordinal encoder since iterative imputer only accepts numerical values\n",
    "# Trees can handle ordinal encoded categorical variables without issue\n",
    "# Using one-hot encoding would increase the number of features too much\n",
    "# However, linear models would require one-hot encoding to avoid implying ordinality\n",
    "# And also for distribution tests later, we need to decode back to original categories\n",
    "\n",
    "# named cat_cols instead of cat_vars to avoid confusion\n",
    "cat_cols = X_train_pre_cleaned.select_dtypes(include=['category']).columns.tolist()\n",
    "# cat_cols = X_train_pre_cleaned.select_dtypes(include=['category', 'object']).columns.tolist() # include 'object' dtype as well\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "X_train_ordinal = X_train_pre_cleaned.copy()\n",
    "X_test_ordinal = X_test_pre_cleaned.copy()\n",
    "X_train_ordinal[cat_cols] = ordinal_encoder.fit_transform(X_train_ordinal[cat_cols])\n",
    "X_test_ordinal[cat_cols] = ordinal_encoder.transform(X_test_ordinal[cat_cols])\n",
    "\n",
    "# transform \"unknown\" into NaN so the imputer imputes them\n",
    "for c in cat_cols:\n",
    "    X_train_ordinal[c] = X_train_ordinal[c].replace(-1, np.nan)\n",
    "    X_test_ordinal[c] = X_test_ordinal[c].replace(-1, np.nan)\n",
    "\n",
    "# not needed but in case want to try distance-based imputers later\n",
    "# num_cols = [c for c in X_train_ordinal.columns if c not in cat_cols]\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = X_train_ordinal.copy()\n",
    "# X_test_scaled = X_test_ordinal.copy()\n",
    "# X_train_scaled[num_cols] = scaler.fit_transform(X_train_scaled[num_cols])\n",
    "# X_test_scaled[num_cols] = scaler.transform(X_test_scaled[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af7da6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IterativeImputer] Completing matrix with shape (7789, 256)\n",
      "[IterativeImputer] Ending imputation round 1/10, elapsed time 54.30\n",
      "[IterativeImputer] Change: 10621.036681443979, scaled tolerance: 68.0 \n",
      "[IterativeImputer] Ending imputation round 2/10, elapsed time 108.37\n",
      "[IterativeImputer] Change: 3403.843214620837, scaled tolerance: 68.0 \n",
      "[IterativeImputer] Ending imputation round 3/10, elapsed time 161.39\n",
      "[IterativeImputer] Change: 3504.9341219513453, scaled tolerance: 68.0 \n",
      "[IterativeImputer] Ending imputation round 4/10, elapsed time 217.70\n",
      "[IterativeImputer] Change: 3520.375820193589, scaled tolerance: 68.0 \n",
      "[IterativeImputer] Ending imputation round 5/10, elapsed time 272.09\n",
      "[IterativeImputer] Change: 2621.9086009891275, scaled tolerance: 68.0 \n",
      "[IterativeImputer] Ending imputation round 6/10, elapsed time 327.70\n",
      "[IterativeImputer] Change: 2785.279687242277, scaled tolerance: 68.0 \n",
      "[IterativeImputer] Ending imputation round 7/10, elapsed time 384.43\n",
      "[IterativeImputer] Change: 2333.067039675238, scaled tolerance: 68.0 \n",
      "[IterativeImputer] Ending imputation round 8/10, elapsed time 443.26\n",
      "[IterativeImputer] Change: 3703.9395620535543, scaled tolerance: 68.0 \n",
      "[IterativeImputer] Ending imputation round 9/10, elapsed time 503.39\n",
      "[IterativeImputer] Change: 2867.1337656017517, scaled tolerance: 68.0 \n",
      "[IterativeImputer] Ending imputation round 10/10, elapsed time 563.60\n",
      "[IterativeImputer] Change: 2765.71324030359, scaled tolerance: 68.0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\victo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\impute\\_iterative.py:825: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# For a random forest imputer, we don't really need a huge number of trees\n",
    "# Imputation is about generating stable estimates, not prediction accuracy\n",
    "# So we can limit the number of trees to speed up computation\n",
    "# Usually n=10-50 is sufficient, but we can go a bit higher if it's unstable\n",
    "\n",
    "# def random_forest_imputer(n_estimators=20, max_iter=3):\n",
    "#     rf_imputer = IterativeImputer(\n",
    "#         estimator=RandomForestRegressor(n_estimators=n_estimators, max_depth=10, n_jobs=-1, random_state=42),\n",
    "#         max_iter=max_iter,\n",
    "#         random_state=42\n",
    "#     )\n",
    "#     return rf_imputer\n",
    "\n",
    "\n",
    "# def random_forest_imputer():\n",
    "#     rf_imputer = IterativeImputer(\n",
    "#         estimator=RandomForestRegressor(\n",
    "#             n_estimators=20,\n",
    "#             max_depth=10,\n",
    "#             min_samples_leaf=30,     # increased for stability\n",
    "#             max_features='sqrt',\n",
    "#             n_jobs=-1,\n",
    "#             random_state=42\n",
    "#         ),\n",
    "#         n_nearest_features=60,       # uses most relevant subset of features\n",
    "#         max_iter=10,\n",
    "#         tol=0.01,                    # balanced tolerance for convergence\n",
    "#         initial_strategy='median',\n",
    "#         random_state=42,\n",
    "#         verbose=2\n",
    "#     )\n",
    "#     return rf_imputer\n",
    "\n",
    "# def random_forest_imputer():\n",
    "#     rf_imputer = IterativeImputer(\n",
    "#         estimator=RandomForestRegressor(\n",
    "#             n_estimators=20,\n",
    "#             max_depth=15,\n",
    "#             min_samples_leaf=20,\n",
    "#             min_samples_split=40,\n",
    "#             max_features='sqrt',\n",
    "#             n_jobs=-1,\n",
    "#             random_state=42\n",
    "#         ),\n",
    "#         n_nearest_features=60,\n",
    "#         max_iter=20,\n",
    "#         tol=0.01,\n",
    "#         initial_strategy='median',\n",
    "#         random_state=42,\n",
    "#         verbose=2\n",
    "#     )\n",
    "#     return rf_imputer\n",
    "\n",
    "# def extra_trees_imputer(n_estimators=20, max_iter=3, verbose=0):\n",
    "#     et_imputer = IterativeImputer(\n",
    "#         estimator=ExtraTreesRegressor(n_estimators=n_estimators, max_depth=10, n_jobs=-1, random_state=42),\n",
    "#         max_iter=max_iter,\n",
    "#         random_state=42,\n",
    "#         verbose=verbose\n",
    "#     )\n",
    "#     return et_imputer\n",
    "\n",
    "# def extra_trees_imputer(n_estimators=20, max_iter=10, verbose=0):\n",
    "#     et_imputer = IterativeImputer(\n",
    "#         estimator=ExtraTreesRegressor(\n",
    "#             n_estimators=n_estimators, \n",
    "#             max_depth=6, \n",
    "#             min_samples_leaf=10,\n",
    "#             n_jobs=-1, \n",
    "#             random_state=42\n",
    "#         ),\n",
    "#         max_iter=max_iter,\n",
    "#         initial_strategy='median',\n",
    "#         random_state=42,\n",
    "#         verbose=verbose\n",
    "#     )\n",
    "#     return et_imputer\n",
    "\n",
    "# def extra_trees_imputer():\n",
    "#     et_imputer = IterativeImputer(\n",
    "#         estimator=ExtraTreesRegressor(\n",
    "#             n_estimators=30,\n",
    "#             max_depth=10,\n",
    "#             min_samples_leaf=30,     # increased for stability\n",
    "#             max_features='sqrt',\n",
    "#             n_jobs=-1,\n",
    "#             random_state=42\n",
    "#         ),\n",
    "#         n_nearest_features=60,       # uses most relevant subset of features\n",
    "#         max_iter=10,\n",
    "#         tol=0.01,                    # 0.001 is too small\n",
    "#         initial_strategy='median',\n",
    "#         random_state=42,\n",
    "#         verbose=2\n",
    "#     )\n",
    "#     return et_imputer\n",
    "\n",
    "\n",
    "def extra_trees_imputer():\n",
    "    et_imputer = IterativeImputer(\n",
    "        estimator=ExtraTreesRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            min_samples_leaf=40,\n",
    "            min_samples_split=60,\n",
    "            max_features='sqrt',\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        ),\n",
    "        n_nearest_features=40,\n",
    "        max_iter=10,\n",
    "        initial_strategy='median',\n",
    "        random_state=42,\n",
    "        imputation_order='ascending',\n",
    "        verbose=2\n",
    "    )\n",
    "    return et_imputer\n",
    "\n",
    "imputer = extra_trees_imputer()\n",
    "# imputer = random_forest_imputer()\n",
    "\n",
    "X_train = imputer.fit_transform(X_train_ordinal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fd1616",
   "metadata": {},
   "source": [
    "> The iterative imputer did not reach the convergence tolerance. However, the change is stabilized and non-divergent after first few iterations, which suffice for practical purpose. The imputation was terminated after 10 rounds as further iteration unlikely will produce any meaningful improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bfd6d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(imputer, \"RESULTS/extra_trees_imputer.pkl\", compress=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c038853f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IterativeImputer] Completing matrix with shape (1948, 256)\n",
      "[IterativeImputer] Ending imputation round 1/10, elapsed time 7.54\n",
      "[IterativeImputer] Ending imputation round 2/10, elapsed time 15.01\n",
      "[IterativeImputer] Ending imputation round 3/10, elapsed time 22.87\n",
      "[IterativeImputer] Ending imputation round 4/10, elapsed time 32.18\n",
      "[IterativeImputer] Ending imputation round 5/10, elapsed time 41.55\n",
      "[IterativeImputer] Ending imputation round 6/10, elapsed time 50.25\n",
      "[IterativeImputer] Ending imputation round 7/10, elapsed time 59.40\n",
      "[IterativeImputer] Ending imputation round 8/10, elapsed time 66.79\n",
      "[IterativeImputer] Ending imputation round 9/10, elapsed time 74.17\n",
      "[IterativeImputer] Ending imputation round 10/10, elapsed time 81.50\n"
     ]
    }
   ],
   "source": [
    "X_test = imputer.transform(X_test_ordinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7521b2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert back to DataFrames\n",
    "X_train_imputed = pd.DataFrame(X_train, columns=X_train_ordinal.columns, index=X_train_ordinal.index)\n",
    "X_test_imputed  = pd.DataFrame(X_test, columns=X_test_ordinal.columns,  index=X_test_ordinal.index)\n",
    "\n",
    "# reverse scaling for numeric columns, if applied\n",
    "# X_train_imputed[num_cols] = scaler.inverse_transform(X_train_imputed[num_cols])\n",
    "# X_test_imputed[num_cols] = scaler.inverse_transform(X_test_imputed[num_cols])\n",
    "\n",
    "# rounding categorical codes to valid range before inverse_transform\n",
    "for i, c in enumerate(cat_cols):\n",
    "    n = len(ordinal_encoder.categories_[i])\n",
    "    X_train_imputed[c] = np.clip(np.rint(X_train_imputed[c]).astype(int), 0, n-1)\n",
    "    X_test_imputed[c] = np.clip(np.rint(X_test_imputed[c]).astype(int),  0, n-1)\n",
    "\n",
    "# restore categorical dtype\n",
    "X_train_imputed[cat_cols] = ordinal_encoder.inverse_transform(X_train_imputed[cat_cols])\n",
    "X_test_imputed[cat_cols] = ordinal_encoder.inverse_transform(X_test_imputed[cat_cols])\n",
    "for col in cat_cols:\n",
    "    X_train_imputed[col] = X_train_imputed[col].astype('category')\n",
    "    X_test_imputed[col] = X_test_imputed[col].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c98ec6",
   "metadata": {},
   "source": [
    "#### Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5d76cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning\n",
    "# SLQ300_Usual_sleep_time_on_weekdays_or_workdays\n",
    "# SLQ320_Usual_sleep_time_on_weekends\n",
    "# 1_very_early: 19-20\n",
    "# 2_early: 21-22\n",
    "# 3_normal: 23-00\n",
    "# 4_late: 01-02\n",
    "# 5_extreme: else\n",
    "\n",
    "\n",
    "cols = [\"P_SLQ__SLQ320_Usual_sleep_time_on_weekends\", \"P_SLQ__SLQ300_Usual_sleep_time_on_weekdays_or_workdays\"]\n",
    "\n",
    "for df in (X_train_imputed, X_test_imputed):\n",
    "    for col in cols:\n",
    "        # convert \"HH:MM\" to rounded hour 0–23\n",
    "        df[col] = pd.to_datetime(df[col], format=\"%H:%M\", errors=\"coerce\").dt.round(\"h\").dt.hour\n",
    "\n",
    "        # binning according to plan above\n",
    "        df[col] = np.select(\n",
    "            [\n",
    "                # use isin instead of between since we already round it anyway and to handle the midnight\n",
    "                df[col].isin([19, 20]),       # 1 very early: 19–20\n",
    "                df[col].isin([21, 22]),       # 2 early: 21–22\n",
    "                df[col].isin([23, 0]),         # 3 normal: 23–00\n",
    "                df[col].isin([1, 2]),          # 4 late: 01–02\n",
    "            ],\n",
    "            [1, 2, 3, 4],\n",
    "            default=5                         # 5 extreme: everything else / NaN\n",
    "        ).astype(\"int64\")   # convert to clear integer, but later it will be preserved as category dtype by json files\n",
    "\n",
    "\n",
    "# Taken out, but kept here for reference\n",
    "# SLQ310_Usual_wake_time_on_weekdays_or_workdays\n",
    "# SLQ330_Usual_wake_time_on_weekends\n",
    "# 1_very_early: 3-4\n",
    "# 2_early: 5-6\n",
    "# 3_normal: 7-8\n",
    "# 4_late: 9-10\n",
    "# 5_extreme: else\n",
    "\n",
    "# SLD012_Sleep_hours_weekdays_or_workdays\n",
    "# SLD013_Sleep_hours_weekends\n",
    "# ≤5 hours -> Very short\n",
    "# >5-<7 hours -> Short\n",
    "# 7-<9 hours -> Normal\n",
    "# ≥9 hours -> Long\n",
    "cols = [\"P_SLQ__SLD012_Sleep_hours_weekdays_or_workdays\", \"P_SLQ__SLD013_Sleep_hours_weekends\"]\n",
    "\n",
    "for df in (X_train_imputed, X_test_imputed):\n",
    "    for col in cols:\n",
    "        # convert to numeric\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "        # binning according to plan above\n",
    "        df[col] = np.select(\n",
    "            [\n",
    "                df[col] <= 5,                   # 1: very short\n",
    "                (df[col] > 5) & (df[col] < 7),  # 2: short\n",
    "                (df[col] >= 7) & (df[col] < 9), # 3: normal\n",
    "                df[col] >= 9                  # 4: long\n",
    "            ],\n",
    "            [1, 2, 3, 4],\n",
    "            default=4                         # 5 extreme: everything else / NaN\n",
    "        ).astype(\"int64\")   # convert to clear integer, but later it will be preserved as category dtype by json files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57191e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"INPUTS/TRAIN\", exist_ok=True)\n",
    "os.makedirs(\"INPUTS/TEST\", exist_ok=True)\n",
    "os.makedirs(\"RESULTS\", exist_ok=True)\n",
    "\n",
    "X_train_imputed.to_parquet(\"INPUTS/TRAIN/X_train.parquet\", index=False)\n",
    "X_test_imputed.to_parquet(\"INPUTS/TEST/X_test.parquet\", index=False)\n",
    "y_train.to_parquet(\"INPUTS/TRAIN/y_train.parquet\", index=False)\n",
    "y_test.to_parquet(\"INPUTS/TEST/y_test.parquet\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
