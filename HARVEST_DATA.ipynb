{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99a554ee",
   "metadata": {},
   "source": [
    "# Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec383f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27a400d",
   "metadata": {},
   "source": [
    "# Harvest Data Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd5a0632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION ---\n",
    "\n",
    "USER_AGENT = \"Mozilla/5.0 (compatible; NHANES-scraper/1.0)\"\n",
    "DEFAULT_TIMEOUT = 30\n",
    "\n",
    "SESSION = requests.Session()\n",
    "SESSION.headers.update({\"User-Agent\": USER_AGENT})\n",
    "\n",
    "DEFAULT_LOGFILE = \"log.txt\"\n",
    "CDC_BASE_URL = \"https://wwwn.cdc.gov\"\n",
    "NHANES_BASE_URL = \"https://wwwn.cdc.gov/nchs/nhanes/\"\n",
    "CYCLE_2017_2020_ROOT_URL = \"https://wwwn.cdc.gov/nchs/nhanes/continuousnhanes/default.aspx?Cycle=2017-2020\" # Only using 2017 - 2020 for now\n",
    "\n",
    "# --- END CONFIGURATION ---\n",
    "\n",
    "# --- HELPER FUNCTIONS ---\n",
    "def log(message, logfile=\"log.txt\"):\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    with open(logfile, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"[{timestamp}] {message}\\n\")\n",
    "\n",
    "def clean_text(text: str):\n",
    "    text = text.strip()\n",
    "    text = text.replace(\" \", \"_\")\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "def clean_href(base_url, a):\n",
    "    try:\n",
    "        if not a:\n",
    "            raise Exception(\"Anchor doesn't exist!\")\n",
    "        if not a.has_attr(\"href\"):\n",
    "            raise Exception(\"Anchor doesn't have href!\")\n",
    "        \n",
    "        href = a[\"href\"].strip()\n",
    "        if href.startswith(\"../\"): \n",
    "            href = base_url + href[3:] \n",
    "        else: \n",
    "            href = base_url + href \n",
    "            return href\n",
    "        \n",
    "        return href\n",
    "    except Exception as e:\n",
    "        log(f\"Expt in clean_href: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def get_soup(url) -> BeautifulSoup:\n",
    "    r = SESSION.get(url, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "# --- END HELPER FUNCTIONS ---\n",
    "\n",
    "# --- SCRAPING FUNCTIONS ---\n",
    "\n",
    "# DDC = Data, Documentation, Codebook\n",
    "def get_ddc_links(nhanes_root_soup: BeautifulSoup) -> dict:\n",
    "    table = nhanes_root_soup.find(\n",
    "        \"div\",\n",
    "        attrs = {'class': 'card-body bg-white no-padding'}\n",
    "    )\n",
    "    a_elements = table.find_all('a')\n",
    "\n",
    "    links = {}\n",
    "    for a_element in a_elements:\n",
    "        name = clean_text(a_element.contents[-1])\n",
    "        link = clean_href(NHANES_BASE_URL, a_element)\n",
    "        links[name] = link\n",
    "    return links\n",
    "\n",
    "\n",
    "def get_category_subset_df(url) -> pd.DataFrame:\n",
    "    soup = get_soup(url)\n",
    "    table = soup.find('table', {'id': 'GridView1'}) # Tables are easily identified by id\n",
    "    # Keep it really simply, get headers, get rows + links, create df\n",
    "\n",
    "    # Get natural headers\n",
    "    headers = []\n",
    "    for th in table.find('thead').find_all('th'):\n",
    "        headers.append(clean_text(th.contents[0]))\n",
    "\n",
    "    # Create array of dictionaries from rows:\n",
    "    rows = []\n",
    "    for tr in table.find(\"tbody\").find_all(\"tr\"):\n",
    "        tds = tr.find_all(\"td\")\n",
    "\n",
    "        # basic text\n",
    "        text_data = [\n",
    "            td.get_text(strip=True) for td in tds\n",
    "        ]\n",
    "\n",
    "        # pick out anchors\n",
    "        doc_a  = tds[2].find(\"a\")\n",
    "        data_a = tds[3].find(\"a\")\n",
    "\n",
    "        # build absolute URLs manually\n",
    "        doc_url  = clean_href(CDC_BASE_URL, doc_a)\n",
    "        data_url = clean_href(CDC_BASE_URL, data_a)\n",
    "\n",
    "        row = dict(zip(headers, text_data))\n",
    "        row[\"doc_url\"] = doc_url\n",
    "        row[\"data_url\"] = data_url\n",
    "\n",
    "        rows.append(row)\n",
    "    \n",
    "    # Create df\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "\n",
    "def download_file(url, file_dir, write_log = True):\n",
    "    r = SESSION.get(url, stream=True)\n",
    "    r.raise_for_status()\n",
    "\n",
    "    file_name = os.path.basename(url)\n",
    "    if not os.path.exists(file_dir):\n",
    "        os.makedirs(file_dir)\n",
    "\n",
    "    save_path = os.path.join(file_dir, file_name)\n",
    "    with open(save_path, 'wb') as f:\n",
    "        for chunk in r.iter_content(chunk_size=16*1024):\n",
    "            f.write(chunk)\n",
    "    \n",
    "    if write_log:\n",
    "        log(f\"{file_name} downloaded successfully to {save_path}\")\n",
    "\n",
    "def download_category_subset(cycle, category_subset):\n",
    "    for row in cycle[category_subset].to_dict(\"records\"):\n",
    "        url = row['data_url']\n",
    "        if not url:\n",
    "            log(f\"{category_subset}: No data url found! Cannot download\")\n",
    "            continue\n",
    "        download_file(url, \"./RAW/DATA/\" + category_subset)\n",
    "        print(f\"Successfully downloaded {url}\")\n",
    "\n",
    "def get_codebook_df(doc_url):\n",
    "    results = []\n",
    "    soup = get_soup(doc_url)\n",
    "    codebook = soup.find('div', {'id': 'Codebook'})\n",
    "    if not codebook:\n",
    "        print(f\"Can't find codebook for {doc_url}!\")\n",
    "        log(f\"Can't find codebook for {doc_url}!\")\n",
    "        return pd.DataFrame()\n",
    "    for div in codebook.find_all('div', {'class': 'pagebreak'}):\n",
    "        code = {}\n",
    "        dl = div.find('dl')\n",
    "        for dt in dl.find_all('dt'):\n",
    "            dd = dt.find_next_sibling('dd')\n",
    "\n",
    "            code_feature_name = clean_text(dt.get_text()).rstrip(\":\")\n",
    "            code_feature_value = dd.get_text()\n",
    "            code[code_feature_name] = code_feature_value\n",
    "            \n",
    "        results.append(code)\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def get_category_subset_codebook(cycle, category_subset):\n",
    "    for row in cycle[category_subset].to_dict(\"records\"):\n",
    "        doc_url = row['doc_url']\n",
    "        if not doc_url:\n",
    "            log(f\"{category_subset}: No doc url found! Cannot get codebook\")\n",
    "            continue\n",
    "        CODEBOOKS_PATH = f\"./RAW/CODEBOOKS/{category_subset}\"\n",
    "\n",
    "        if not os.path.exists(CODEBOOKS_PATH):\n",
    "            os.makedirs(CODEBOOKS_PATH)\n",
    "        url = row['data_url']\n",
    "        if not url:\n",
    "            log(f\"{category_subset}: No data url found! Cannot download\")\n",
    "            continue\n",
    "        file_name = os.path.basename(url).split('.')[0]\n",
    "        doc_df = get_codebook_df(doc_url)\n",
    "        doc_df['data_file'] = file_name\n",
    "        doc_df.to_csv(CODEBOOKS_PATH + f\"/{file_name}_codebook.csv\", index = False)\n",
    "# --- END SCRAPING FUNCTIONS ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb9333a",
   "metadata": {},
   "source": [
    "# Perform Harvesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3704f4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get links from the main root cycle page\n",
    "links = get_ddc_links(get_soup(CYCLE_2017_2020_ROOT_URL))\n",
    "\n",
    "# Create dataframes with subcategory information\n",
    "cycle_2017_2020_dfs = {}\n",
    "for name, link in links.items():\n",
    "    print(name, link)\n",
    "    cycle_2017_2020_dfs[name] = get_category_subset_df(link)\n",
    "\n",
    "# For subcategory, download the data\n",
    "for category_subset in cycle_2017_2020_dfs.keys():\n",
    "    download_category_subset(cycle_2017_2020_dfs, category_subset)\n",
    "\n",
    "# For subcategory, get the codebooks\n",
    "for category_subset in cycle_2017_2020_dfs.keys():\n",
    "    get_category_subset_codebook(cycle_2017_2020_dfs, category_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2ee8d4",
   "metadata": {},
   "source": [
    "# Merge harvested data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30013c2f",
   "metadata": {},
   "source": [
    "Merging all files together to base `P_GHB.xpt`:\n",
    "- P_GHB contains our target, so this will be the base of our merging operations\n",
    "- ⚠️ Skipping 9 specific files with duplicate `SEQN`:\n",
    "    - dietary_data: The dietary data large contains some diet information over the course of two tracked days, this information is not particularly insightful for our case, since diabetes is a longform developing disease.\n",
    "        - `P_DR1IFF.xpt`\n",
    "        - `P_DR2IFF.xpt`\n",
    "        - `P_DS1IDS.xpt`\n",
    "        - `P_DS2IDS.xpt`\n",
    "        - `P_DSQIDS.xpt`\n",
    "    - examination_data: These files deal with audio sensor data, which is not useful for our use case.\n",
    "        - `P_AUXAR.xpt`\n",
    "        - `P_AUXTYM.xpt`\n",
    "        - `P_AUXWBR.xpt`\n",
    "    - questionnaire_data: This data needs to undergo transformation before being usable. We first transform this dataset before merging.\n",
    "        - `P_RXQ_RX.xpt`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6e6b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path(\"RAW/DATA\")\n",
    "xpt_files = list(root.rglob(\"*.xpt\"))\n",
    "\n",
    "P_GHB_path = 'RAW/DATA/laboratory_data/P_GHB.xpt'\n",
    "P_GHB = pd.read_sas(P_GHB_path, format='xport', encoding = 'utf-8')\n",
    "\n",
    "base = P_GHB.copy()\n",
    "base['SEQN'] = pd.to_numeric(base['SEQN']).astype('Int64')\n",
    "base_idxed = base.set_index('SEQN')\n",
    "\n",
    "used_cols = set(base_idxed.columns) # Tracking used column names for collisions\n",
    "\n",
    "dfs = []  # all other dfs, already indexed by SEQN\n",
    "for p in xpt_files:\n",
    "    filename = os.path.basename(p)\n",
    "    stem = Path(filename).stem\n",
    "    print(f\"Loading: {filename}\")\n",
    "\n",
    "    # Read with fallback encoding\n",
    "    try:\n",
    "        df = pd.read_sas(p, format=\"xport\", encoding=\"utf-8\")\n",
    "    except UnicodeDecodeError:\n",
    "        df = pd.read_sas(p, format=\"xport\", encoding=\"cp1252\")\n",
    "\n",
    "    if 'SEQN' not in df.columns:\n",
    "        print(f\"\\tSkipping {filename}, missing SEQN\")\n",
    "        continue\n",
    "\n",
    "    \n",
    "    df['SEQN'] = pd.to_numeric(df['SEQN']).astype('Int64')\n",
    "\n",
    "    dup_counts = df['SEQN'].value_counts()\n",
    "    dup_counts = dup_counts[dup_counts > 1]\n",
    "    if not dup_counts.empty:\n",
    "        print(f\"\\tSkipping {filename}. Found {dup_counts.size} duplicated SEQN values\")\n",
    "        continue\n",
    "    df = df.set_index('SEQN')\n",
    "    \n",
    "    # Renaming only colliding columns (prefixing with filename)\n",
    "    rename_mapper = {}\n",
    "    for col in df.columns:\n",
    "        if col in used_cols:\n",
    "            rename_mapper[col] = f\"{filename}_{col}\"\n",
    "    if rename_mapper:\n",
    "        df = df.rename(columns=rename_mapper)\n",
    "    used_cols.update(df.columns) # Update used set\n",
    "\n",
    "    dfs.append(df)\n",
    "\n",
    "# ---- One big combine ----\n",
    "# Concatenate all other files horizontally (align on SEQN once)\n",
    "others = pd.concat(dfs, axis=1, copy=False)\n",
    "\n",
    "# Single join to the base\n",
    "merged = base_idxed.join(others, how='left')\n",
    "merged = merged.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd6eb6e",
   "metadata": {},
   "source": [
    "Save merged to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdcaed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"./PROCESSED/DATA\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "save_path = directory + \"/merged.csv\"\n",
    "merged.to_csv(save_path, index = False)\n",
    "print(f\"Saved merged.csv to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6ac36ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from io import StringIO\n",
    "\n",
    "\n",
    "# INVALID_PATTERN = re.compile(\n",
    "#     r\"(refused|don['']?t\\s*know|missing|blank but applicable|\"\n",
    "#     r\"(can|could)\\s*not\\s*assess|unknown|not\\s*ascertained)\",\n",
    "#     re.IGNORECASE\n",
    "# )\n",
    "\n",
    "# chg to non-capturing groups (?:...) instead of capturing groups (...)\n",
    "INVALID_PATTERN = re.compile(\n",
    "    r\"(?:refused|don['']?t\\s*know|missing|blank but applicable|\"\n",
    "    r\"(?:can|could)\\s*not\\s*assess|unknown|not\\s*ascertained)\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "# get invalid code\n",
    "def get_invalid_codes(doc_url):\n",
    "    result = {}\n",
    "    soup = get_soup(doc_url)\n",
    "    codebook = soup.find('div', id='Codebook')\n",
    "    if not codebook:\n",
    "        return result\n",
    "\n",
    "    for block in codebook.select('div.pagebreak'):\n",
    "        try:\n",
    "            # variable name\n",
    "            var_dt = block.find('dt', string=re.compile(r'Variable Name', re.I))\n",
    "            var_name = var_dt.find_next_sibling('dd').get_text(strip=True)\n",
    "\n",
    "            # Fix: Wrap HTML string in StringIO\n",
    "            tables = pd.read_html(StringIO(str(block)))\n",
    "            for df in tables:\n",
    "                if \"Code or Value\" in df.columns and \"Value Description\" in df.columns:\n",
    "                    # rows with invalid desc\n",
    "                    # regex warning handled by using non-capturing groups above\n",
    "                    invalid = df[\"Value Description\"].astype(str).str.contains(INVALID_PATTERN, na=False)\n",
    "                    if invalid.any():\n",
    "                        codes = df.loc[invalid, \"Code or Value\"].astype(str).str.strip().tolist()\n",
    "                        result[var_name] = codes\n",
    "                    break\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return result\n",
    "\n",
    "# invalid_codes from all pages\n",
    "def build_invalid_map(category_df):\n",
    "    invalid_map = {}\n",
    "    for url in category_df[\"doc_url\"].dropna():\n",
    "        try:\n",
    "            invalid_map.update(get_invalid_codes(url))\n",
    "        except:\n",
    "            pass\n",
    "    return invalid_map\n",
    "\n",
    "# run\n",
    "subset_url = \"https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Questionnaire&Cycle=2017-2020\"\n",
    "subset_df = get_category_subset_df(subset_url)\n",
    "invalid_map = build_invalid_map(subset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "485c59c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved invalid_map_questionnaire_2017_2020.csv to ./PROCESSED/DATA/invalid_map_questionnaire_2017_2020.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the invalid map to a CSV file\n",
    "\n",
    "df_invalid_map = pd.DataFrame([\n",
    "    {'variable': var, 'invalid_code': code}\n",
    "    for var, codes in invalid_map.items()\n",
    "    for code in codes\n",
    "])\n",
    "\n",
    "directory = \"./PROCESSED/DATA\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "save_path = directory + \"/invalid_map_questionnaire_2017_2020.csv\"\n",
    "df_invalid_map.to_csv(save_path, index = False)\n",
    "print(f\"Saved invalid_map_questionnaire_2017_2020.csv to {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
